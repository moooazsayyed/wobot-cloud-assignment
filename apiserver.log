
==> Audit <==
|----------------|--------------------------------|-----------|-------------|---------|---------------------|---------------------|
|    Command     |              Args              |  Profile  |    User     | Version |     Start Time      |      End Time       |
|----------------|--------------------------------|-----------|-------------|---------|---------------------|---------------------|
| addons         | enable ingress                 | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 00:54 IST | 15 May 24 00:54 IST |
| service        | rails-ingress --url            | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 00:54 IST |                     |
| service        | rails-ingress --url            | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 00:56 IST |                     |
| service        | list                           | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 00:57 IST | 15 May 24 00:57 IST |
| service        | rails --url                    | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 00:58 IST | 15 May 24 00:58 IST |
| service        | list                           | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:00 IST | 15 May 24 01:00 IST |
| service        | rails --url                    | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:01 IST | 15 May 24 01:02 IST |
| service        | list                           | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:02 IST | 15 May 24 01:02 IST |
| ip             |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:02 IST | 15 May 24 01:02 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:03 IST | 15 May 24 01:05 IST |
| service        | list                           | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:07 IST | 15 May 24 01:07 IST |
| ip             |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:08 IST | 15 May 24 01:08 IST |
| service        | rails-ingress --url            | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:11 IST |                     |
| service        | rails --url                    | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:11 IST | 15 May 24 01:14 IST |
| ip             |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:15 IST | 15 May 24 01:15 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:19 IST | 15 May 24 01:21 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:28 IST | 15 May 24 01:30 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:31 IST | 15 May 24 01:35 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:35 IST |                     |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:41 IST | 15 May 24 01:42 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:50 IST | 15 May 24 01:51 IST |
| service        | rails-ingress --url            | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:52 IST |                     |
| service        | rails --url                    | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:52 IST | 15 May 24 01:56 IST |
| ip             |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:53 IST | 15 May 24 01:53 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:53 IST | 15 May 24 01:59 IST |
| ip             |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 01:59 IST | 15 May 24 01:59 IST |
| ip             |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 02:03 IST | 15 May 24 02:03 IST |
| service        | rails --url                    | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 02:03 IST | 15 May 24 02:08 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 02:14 IST | 15 May 24 02:15 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 02:16 IST | 15 May 24 02:20 IST |
| service        | rails --url                    | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 02:19 IST | 15 May 24 02:22 IST |
| tunnel         |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 15 May 24 02:21 IST |                     |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 16 May 24 13:01 IST | 16 May 24 13:02 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 16 May 24 15:06 IST | 16 May 24 15:07 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 16 May 24 17:41 IST | 16 May 24 17:42 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 17 May 24 01:25 IST | 17 May 24 01:26 IST |
| ssh            |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 17 May 24 02:22 IST |                     |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 17 May 24 02:30 IST | 17 May 24 02:31 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 17 May 24 11:40 IST |                     |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 17 May 24 11:41 IST | 17 May 24 11:42 IST |
| start          | --kubernetes-version v1.25.14  | minikube  | MOOAZ-PC\hp | v1.33.0 | 18 May 24 18:20 IST |                     |
| start          | -p minikube2                   | minikube2 | MOOAZ-PC\hp | v1.33.0 | 18 May 24 18:21 IST |                     |
|                | --kubernetes-version=v1.25.14  |           |             |         |                     |                     |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 18 May 24 18:24 IST | 18 May 24 18:25 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 22 May 24 00:41 IST | 22 May 24 00:42 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jun 24 15:41 IST | 04 Jun 24 15:43 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jun 24 20:00 IST | 04 Jun 24 20:01 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jun 24 23:36 IST | 04 Jun 24 23:40 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 15:52 IST | 04 Jul 24 15:54 IST |
| stop           |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 17:23 IST | 04 Jul 24 17:24 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 17:24 IST | 04 Jul 24 17:25 IST |
| ssh            |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 17:25 IST | 04 Jul 24 17:27 IST |
| start          |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:18 IST | 04 Jul 24 20:20 IST |
| ssh            |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:27 IST | 04 Jul 24 20:29 IST |
| ssh            |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:27 IST | 04 Jul 24 20:29 IST |
| stop           |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:33 IST | 04 Jul 24 20:34 IST |
| start          | --memory=4096 --cpus=4         | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:36 IST |                     |
| start          | --memory=3000 --cpus=4         | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:38 IST | 04 Jul 24 20:39 IST |
| update-check   |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:40 IST | 04 Jul 24 20:40 IST |
| update-context |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:41 IST | 04 Jul 24 20:41 IST |
| update-check   |                                | minikube  | MOOAZ-PC\hp | v1.33.0 | 04 Jul 24 20:41 IST | 04 Jul 24 20:41 IST |
|----------------|--------------------------------|-----------|-------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/07/04 20:38:07
Running on machine: mooaz-pc
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0704 20:38:07.085896   10176 out.go:291] Setting OutFile to fd 108 ...
I0704 20:38:07.089420   10176 out.go:343] isatty.IsTerminal(108) = true
I0704 20:38:07.089420   10176 out.go:304] Setting ErrFile to fd 112...
I0704 20:38:07.089420   10176 out.go:343] isatty.IsTerminal(112) = true
I0704 20:38:07.120932   10176 out.go:298] Setting JSON to false
I0704 20:38:07.129039   10176 start.go:129] hostinfo: {"hostname":"mooaz-pc","uptime":2946,"bootTime":1720102740,"procs":249,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.3810 Build 22621.3810","kernelVersion":"10.0.22621.3810 Build 22621.3810","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"59e976e0-b1e3-4eaf-9e84-3ca7a8c62a5d"}
W0704 20:38:07.129092   10176 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0704 20:38:07.132345   10176 out.go:177] 😄  minikube v1.33.0 on Microsoft Windows 11 Home Single Language 10.0.22621.3810 Build 22621.3810
I0704 20:38:07.135011   10176 notify.go:220] Checking for updates...
I0704 20:38:07.138153   10176 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0704 20:38:07.142729   10176 driver.go:392] Setting default libvirt URI to qemu:///system
I0704 20:38:07.752370   10176 docker.go:122] docker version: linux-26.1.4:Docker Desktop 4.31.1 (153621)
I0704 20:38:07.770436   10176 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0704 20:38:09.198009   10176 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.4275728s)
I0704 20:38:09.201132   10176 info.go:266] docker info: {ID:4af4e05d-0057-4b48-ac5b-f46626e67b92 Containers:28 ContainersRunning:9 ContainersPaused:0 ContainersStopped:19 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:115 OomKillDisable:true NGoroutines:232 SystemTime:2024-07-04 15:08:09.094782166 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:4004683776 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:iuhx1sy9j7e5aqs4mb3i4stcw NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:iuhx1sy9j7e5aqs4mb3i4stcw]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0704 20:38:09.202167   10176 out.go:177] ✨  Using the docker driver based on existing profile
I0704 20:38:09.203214   10176 start.go:297] selected driver: docker
I0704 20:38:09.203214   10176 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\hp:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0704 20:38:09.203214   10176 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0704 20:38:09.290843   10176 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0704 20:38:09.679154   10176 info.go:266] docker info: {ID:4af4e05d-0057-4b48-ac5b-f46626e67b92 Containers:28 ContainersRunning:9 ContainersPaused:0 ContainersStopped:19 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:115 OomKillDisable:true NGoroutines:232 SystemTime:2024-07-04 15:08:09.500421504 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:8 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:4004683776 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:iuhx1sy9j7e5aqs4mb3i4stcw NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:iuhx1sy9j7e5aqs4mb3i4stcw]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
W0704 20:38:09.680739   10176 out.go:239] ❗  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
W0704 20:38:09.681268   10176 out.go:239] ❗  You cannot change the CPUs for an existing minikube cluster. Please first delete the cluster.
I0704 20:38:09.684444   10176 cni.go:84] Creating CNI manager for ""
I0704 20:38:09.684987   10176 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0704 20:38:09.684987   10176 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\hp:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0704 20:38:09.687080   10176 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0704 20:38:09.690135   10176 cache.go:121] Beginning downloading kic base image for docker with docker
I0704 20:38:09.691631   10176 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0704 20:38:09.694329   10176 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0704 20:38:09.694329   10176 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0704 20:38:09.694876   10176 preload.go:147] Found local preload: C:\Users\hp\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0704 20:38:09.694876   10176 cache.go:56] Caching tarball of preloaded images
I0704 20:38:09.696649   10176 preload.go:173] Found C:\Users\hp\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0704 20:38:09.696649   10176 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0704 20:38:09.697374   10176 profile.go:143] Saving config to C:\Users\hp\.minikube\profiles\minikube\config.json ...
I0704 20:38:09.754970   10176 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0704 20:38:09.755496   10176 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0704 20:38:09.756542   10176 cache.go:194] Successfully downloaded all kic artifacts
I0704 20:38:09.758133   10176 start.go:360] acquireMachinesLock for minikube: {Name:mkde6b11429c6a385f5d9257d131216748a248e0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0704 20:38:09.758666   10176 start.go:364] duration metric: took 533µs to acquireMachinesLock for "minikube"
I0704 20:38:09.758666   10176 start.go:96] Skipping create...Using existing machine configuration
I0704 20:38:09.759192   10176 fix.go:54] fixHost starting: 
I0704 20:38:09.774636   10176 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 20:38:09.825098   10176 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0704 20:38:09.825098   10176 fix.go:138] unexpected machine state, will restart: <nil>
I0704 20:38:09.826149   10176 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0704 20:38:09.834116   10176 cli_runner.go:164] Run: docker start minikube
I0704 20:38:11.940019   10176 cli_runner.go:217] Completed: docker start minikube: (2.105903s)
I0704 20:38:11.948950   10176 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 20:38:11.998314   10176 kic.go:430] container "minikube" state is running.
I0704 20:38:12.010135   10176 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0704 20:38:12.060778   10176 profile.go:143] Saving config to C:\Users\hp\.minikube\profiles\minikube\config.json ...
I0704 20:38:12.061836   10176 machine.go:94] provisionDockerMachine start ...
I0704 20:38:12.067761   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:12.126752   10176 main.go:141] libmachine: Using SSH client type: native
I0704 20:38:12.131357   10176 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x123a1c0] 0x123cda0 <nil>  [] 0s} 127.0.0.1 61952 <nil> <nil>}
I0704 20:38:12.131357   10176 main.go:141] libmachine: About to run SSH command:
hostname
I0704 20:38:12.135188   10176 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0704 20:38:15.613004   10176 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0704 20:38:15.613511   10176 ubuntu.go:169] provisioning hostname "minikube"
I0704 20:38:15.621391   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:15.702344   10176 main.go:141] libmachine: Using SSH client type: native
I0704 20:38:15.703405   10176 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x123a1c0] 0x123cda0 <nil>  [] 0s} 127.0.0.1 61952 <nil> <nil>}
I0704 20:38:15.703405   10176 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0704 20:38:16.200387   10176 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0704 20:38:16.210399   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:16.261784   10176 main.go:141] libmachine: Using SSH client type: native
I0704 20:38:16.262445   10176 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x123a1c0] 0x123cda0 <nil>  [] 0s} 127.0.0.1 61952 <nil> <nil>}
I0704 20:38:16.262445   10176 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0704 20:38:16.984215   10176 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0704 20:38:16.984721   10176 ubuntu.go:175] set auth options {CertDir:C:\Users\hp\.minikube CaCertPath:C:\Users\hp\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\hp\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\hp\.minikube\machines\server.pem ServerKeyPath:C:\Users\hp\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\hp\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\hp\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\hp\.minikube}
I0704 20:38:16.984721   10176 ubuntu.go:177] setting up certificates
I0704 20:38:16.984721   10176 provision.go:84] configureAuth start
I0704 20:38:17.002597   10176 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0704 20:38:17.047102   10176 provision.go:143] copyHostCerts
I0704 20:38:17.049247   10176 exec_runner.go:144] found C:\Users\hp\.minikube/ca.pem, removing ...
I0704 20:38:17.049247   10176 exec_runner.go:203] rm: C:\Users\hp\.minikube\ca.pem
I0704 20:38:17.050399   10176 exec_runner.go:151] cp: C:\Users\hp\.minikube\certs\ca.pem --> C:\Users\hp\.minikube/ca.pem (1066 bytes)
I0704 20:38:17.053829   10176 exec_runner.go:144] found C:\Users\hp\.minikube/cert.pem, removing ...
I0704 20:38:17.053829   10176 exec_runner.go:203] rm: C:\Users\hp\.minikube\cert.pem
I0704 20:38:17.054402   10176 exec_runner.go:151] cp: C:\Users\hp\.minikube\certs\cert.pem --> C:\Users\hp\.minikube/cert.pem (1111 bytes)
I0704 20:38:17.056809   10176 exec_runner.go:144] found C:\Users\hp\.minikube/key.pem, removing ...
I0704 20:38:17.056809   10176 exec_runner.go:203] rm: C:\Users\hp\.minikube\key.pem
I0704 20:38:17.058534   10176 exec_runner.go:151] cp: C:\Users\hp\.minikube\certs\key.pem --> C:\Users\hp\.minikube/key.pem (1679 bytes)
I0704 20:38:17.059467   10176 provision.go:117] generating server cert: C:\Users\hp\.minikube\machines\server.pem ca-key=C:\Users\hp\.minikube\certs\ca.pem private-key=C:\Users\hp\.minikube\certs\ca-key.pem org=hp.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0704 20:38:17.272201   10176 provision.go:177] copyRemoteCerts
I0704 20:38:17.293812   10176 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0704 20:38:17.300686   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:17.338798   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:17.496409   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1066 bytes)
I0704 20:38:17.546154   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\machines\server.pem --> /etc/docker/server.pem (1168 bytes)
I0704 20:38:17.628754   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0704 20:38:17.707819   10176 provision.go:87] duration metric: took 722.5908ms to configureAuth
I0704 20:38:17.707819   10176 ubuntu.go:193] setting minikube options for container-runtime
I0704 20:38:17.708901   10176 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0704 20:38:17.718156   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:17.779786   10176 main.go:141] libmachine: Using SSH client type: native
I0704 20:38:17.780290   10176 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x123a1c0] 0x123cda0 <nil>  [] 0s} 127.0.0.1 61952 <nil> <nil>}
I0704 20:38:17.780290   10176 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0704 20:38:17.992999   10176 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0704 20:38:17.992999   10176 ubuntu.go:71] root file system type: overlay
I0704 20:38:17.992999   10176 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0704 20:38:18.007236   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:18.060967   10176 main.go:141] libmachine: Using SSH client type: native
I0704 20:38:18.061566   10176 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x123a1c0] 0x123cda0 <nil>  [] 0s} 127.0.0.1 61952 <nil> <nil>}
I0704 20:38:18.061566   10176 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0704 20:38:18.267141   10176 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0704 20:38:18.275923   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:18.332327   10176 main.go:141] libmachine: Using SSH client type: native
I0704 20:38:18.332943   10176 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x123a1c0] 0x123cda0 <nil>  [] 0s} 127.0.0.1 61952 <nil> <nil>}
I0704 20:38:18.333018   10176 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0704 20:38:18.636937   10176 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0704 20:38:18.636937   10176 machine.go:97] duration metric: took 6.5751016s to provisionDockerMachine
I0704 20:38:18.637442   10176 start.go:293] postStartSetup for "minikube" (driver="docker")
I0704 20:38:18.637442   10176 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0704 20:38:18.677849   10176 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0704 20:38:18.698373   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:18.756326   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:18.940475   10176 ssh_runner.go:195] Run: cat /etc/os-release
I0704 20:38:18.952356   10176 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0704 20:38:18.952356   10176 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0704 20:38:18.952356   10176 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0704 20:38:18.952356   10176 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0704 20:38:18.953977   10176 filesync.go:126] Scanning C:\Users\hp\.minikube\addons for local assets ...
I0704 20:38:18.954504   10176 filesync.go:126] Scanning C:\Users\hp\.minikube\files for local assets ...
I0704 20:38:18.955552   10176 start.go:296] duration metric: took 318.1095ms for postStartSetup
I0704 20:38:19.005281   10176 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0704 20:38:19.024189   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:19.072068   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:19.318860   10176 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0704 20:38:19.324804   10176 fix.go:56] duration metric: took 9.5656122s for fixHost
I0704 20:38:19.324804   10176 start.go:83] releasing machines lock for "minikube", held for 9.5661385s
I0704 20:38:19.333375   10176 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0704 20:38:19.421479   10176 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0704 20:38:19.448997   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:19.470282   10176 ssh_runner.go:195] Run: cat /version.json
I0704 20:38:19.491183   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:19.539847   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:19.576808   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:20.373944   10176 ssh_runner.go:195] Run: systemctl --version
I0704 20:38:20.442824   10176 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0704 20:38:20.504954   10176 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0704 20:38:20.556314   10176 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0704 20:38:20.601070   10176 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0704 20:38:20.647252   10176 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0704 20:38:20.647252   10176 start.go:494] detecting cgroup driver to use...
I0704 20:38:20.647252   10176 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0704 20:38:20.653082   10176 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0704 20:38:20.824104   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0704 20:38:20.926199   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0704 20:38:20.954695   10176 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0704 20:38:20.996024   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0704 20:38:21.056569   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0704 20:38:21.377867   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0704 20:38:21.431074   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0704 20:38:21.473930   10176 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0704 20:38:21.527382   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0704 20:38:21.586204   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0704 20:38:21.635512   10176 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0704 20:38:21.686866   10176 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0704 20:38:21.755631   10176 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0704 20:38:21.797889   10176 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 20:38:22.151593   10176 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0704 20:38:24.685769   10176 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (2.5341758s)
I0704 20:38:24.685769   10176 start.go:494] detecting cgroup driver to use...
I0704 20:38:24.685769   10176 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0704 20:38:24.726440   10176 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0704 20:38:24.767054   10176 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0704 20:38:24.798627   10176 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0704 20:38:24.830904   10176 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0704 20:38:24.966179   10176 ssh_runner.go:195] Run: which cri-dockerd
I0704 20:38:25.010319   10176 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0704 20:38:25.034819   10176 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0704 20:38:25.098171   10176 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0704 20:38:25.414155   10176 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0704 20:38:25.595423   10176 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0704 20:38:25.595931   10176 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0704 20:38:25.673584   10176 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 20:38:25.902799   10176 ssh_runner.go:195] Run: sudo systemctl restart docker
I0704 20:38:42.754482   10176 ssh_runner.go:235] Completed: sudo systemctl restart docker: (16.8516836s)
I0704 20:38:42.769878   10176 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0704 20:38:42.797149   10176 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0704 20:38:42.823095   10176 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0704 20:38:42.847252   10176 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0704 20:38:42.951544   10176 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0704 20:38:43.057364   10176 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 20:38:43.173547   10176 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0704 20:38:43.200804   10176 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0704 20:38:43.229837   10176 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 20:38:43.346025   10176 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0704 20:38:44.290247   10176 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0704 20:38:44.302698   10176 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0704 20:38:44.308605   10176 start.go:562] Will wait 60s for crictl version
I0704 20:38:44.322197   10176 ssh_runner.go:195] Run: which crictl
I0704 20:38:44.340520   10176 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0704 20:38:44.908580   10176 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0704 20:38:44.913557   10176 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0704 20:38:45.268456   10176 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0704 20:38:45.324855   10176 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0704 20:38:45.330577   10176 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0704 20:38:45.606902   10176 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0704 20:38:45.622480   10176 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0704 20:38:45.630722   10176 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0704 20:38:45.646941   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0704 20:38:45.692418   10176 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\hp:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0704 20:38:45.692931   10176 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0704 20:38:45.698469   10176 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0704 20:38:45.834145   10176 docker.go:685] Got preloaded images: -- stdout --
grafana/grafana:latest
prom/prometheus:latest
quay.io/prometheus/prometheus:v2.53.0
quay.io/prometheus/node-exporter:v1.8.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
rails_docker_compose_psql-main-demo-web:latest
quay.io/argoproj/argocd:v2.11.0
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
quay.io/prometheus/pushgateway:v1.8.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
registry.k8s.io/ingress-nginx/controller:<none>
quay.io/prometheus/alertmanager:v0.27.0
registry.k8s.io/etcd:3.5.12-0
ghcr.io/dexidp/dex:v2.38.0
redis:7.0.14-alpine
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/events:<none>
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/resolvers:<none>
gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:<none>
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:<none>
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:<none>
busybox:latest
registry.k8s.io/pause:3.9
postgres:14.2-alpine
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0704 20:38:45.834145   10176 docker.go:615] Images already preloaded, skipping extraction
I0704 20:38:45.842754   10176 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0704 20:38:45.869564   10176 docker.go:685] Got preloaded images: -- stdout --
grafana/grafana:latest
quay.io/prometheus/prometheus:v2.53.0
prom/prometheus:latest
quay.io/prometheus/node-exporter:v1.8.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.74.0
rails_docker_compose_psql-main-demo-web:latest
quay.io/argoproj/argocd:v2.11.0
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
quay.io/prometheus/pushgateway:v1.8.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
registry.k8s.io/ingress-nginx/controller:<none>
quay.io/prometheus/alertmanager:v0.27.0
registry.k8s.io/etcd:3.5.12-0
ghcr.io/dexidp/dex:v2.38.0
redis:7.0.14-alpine
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/events:<none>
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:<none>
gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:<none>
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/resolvers:<none>
gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:<none>
busybox:latest
registry.k8s.io/pause:3.9
postgres:14.2-alpine
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0704 20:38:45.870101   10176 cache_images.go:84] Images are preloaded, skipping loading
I0704 20:38:45.870101   10176 kubeadm.go:928] updating node { 192.168.58.2 8443 v1.30.0 docker true true} ...
I0704 20:38:45.872200   10176 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0704 20:38:45.879386   10176 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0704 20:38:46.880020   10176 cni.go:84] Creating CNI manager for ""
I0704 20:38:46.880020   10176 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0704 20:38:46.880020   10176 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0704 20:38:46.880020   10176 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0704 20:38:46.881052   10176 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0704 20:38:46.894075   10176 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0704 20:38:46.922466   10176 binaries.go:44] Found k8s binaries, skipping transfer
I0704 20:38:46.936396   10176 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0704 20:38:46.945919   10176 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0704 20:38:46.974444   10176 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0704 20:38:47.012937   10176 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0704 20:38:47.055288   10176 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0704 20:38:47.061417   10176 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0704 20:38:47.092155   10176 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 20:38:47.277981   10176 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0704 20:38:47.323927   10176 certs.go:68] Setting up C:\Users\hp\.minikube\profiles\minikube for IP: 192.168.58.2
I0704 20:38:47.323927   10176 certs.go:194] generating shared ca certs ...
I0704 20:38:47.323927   10176 certs.go:226] acquiring lock for ca certs: {Name:mk27c3b623570a460b6f660ea8c2c6e734c45e5e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 20:38:47.324945   10176 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\hp\.minikube\ca.key
I0704 20:38:47.325983   10176 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\hp\.minikube\proxy-client-ca.key
I0704 20:38:47.326497   10176 certs.go:256] generating profile certs ...
I0704 20:38:47.332163   10176 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\hp\.minikube\profiles\minikube\client.key
I0704 20:38:47.354287   10176 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\hp\.minikube\profiles\minikube\apiserver.key.502bbb95
I0704 20:38:47.356635   10176 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\hp\.minikube\profiles\minikube\proxy-client.key
I0704 20:38:47.361154   10176 certs.go:484] found cert: C:\Users\hp\.minikube\certs\ca-key.pem (1679 bytes)
I0704 20:38:47.369122   10176 certs.go:484] found cert: C:\Users\hp\.minikube\certs\ca.pem (1066 bytes)
I0704 20:38:47.380329   10176 certs.go:484] found cert: C:\Users\hp\.minikube\certs\cert.pem (1111 bytes)
I0704 20:38:47.386374   10176 certs.go:484] found cert: C:\Users\hp\.minikube\certs\key.pem (1679 bytes)
I0704 20:38:47.460752   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0704 20:38:47.622189   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0704 20:38:47.863540   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0704 20:38:48.136703   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0704 20:38:48.317116   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0704 20:38:48.371306   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0704 20:38:48.631533   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0704 20:38:48.806140   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0704 20:38:48.927086   10176 ssh_runner.go:362] scp C:\Users\hp\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0704 20:38:48.977705   10176 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0704 20:38:49.001460   10176 ssh_runner.go:195] Run: openssl version
I0704 20:38:49.049323   10176 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0704 20:38:49.087396   10176 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0704 20:38:49.132997   10176 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 14 15:16 /usr/share/ca-certificates/minikubeCA.pem
I0704 20:38:49.134029   10176 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0704 20:38:49.173932   10176 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0704 20:38:49.197777   10176 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0704 20:38:49.204329   10176 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0704 20:38:49.231838   10176 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0704 20:38:49.251740   10176 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0704 20:38:49.261389   10176 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0704 20:38:49.272706   10176 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0704 20:38:49.282042   10176 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0704 20:38:49.321552   10176 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\hp:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0704 20:38:49.337022   10176 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0704 20:38:49.488004   10176 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0704 20:38:49.523692   10176 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0704 20:38:49.523692   10176 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0704 20:38:49.523867   10176 kubeadm.go:587] restartPrimaryControlPlane start ...
I0704 20:38:49.553217   10176 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0704 20:38:49.591410   10176 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0704 20:38:49.600864   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0704 20:38:49.666527   10176 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\hp\.kube\config
I0704 20:38:49.667060   10176 kubeconfig.go:62] C:\Users\hp\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0704 20:38:49.669203   10176 lock.go:35] WriteFile acquiring C:\Users\hp\.kube\config: {Name:mk442be8c0dbd93ff2b6a29b365b789337551f63 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 20:38:49.735658   10176 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0704 20:38:49.750076   10176 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0704 20:38:49.750076   10176 kubeadm.go:591] duration metric: took 226.2089ms to restartPrimaryControlPlane
I0704 20:38:49.750076   10176 kubeadm.go:393] duration metric: took 428.5237ms to StartCluster
I0704 20:38:49.750076   10176 settings.go:142] acquiring lock: {Name:mkae0fd61fd69d226cf369baef917257142a4fc8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 20:38:49.750076   10176 settings.go:150] Updating kubeconfig:  C:\Users\hp\.kube\config
I0704 20:38:49.751624   10176 lock.go:35] WriteFile acquiring C:\Users\hp\.kube\config: {Name:mk442be8c0dbd93ff2b6a29b365b789337551f63 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0704 20:38:49.754294   10176 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0704 20:38:49.755343   10176 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0704 20:38:49.755343   10176 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0704 20:38:49.755343   10176 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0704 20:38:49.755343   10176 addons.go:69] Setting ingress=true in profile "minikube"
I0704 20:38:49.755343   10176 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0704 20:38:49.757589   10176 out.go:177] 🔎  Verifying Kubernetes components...
I0704 20:38:49.755870   10176 addons.go:234] Setting addon ingress=true in "minikube"
W0704 20:38:49.757589   10176 addons.go:243] addon ingress should already be in state true
I0704 20:38:49.755870   10176 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0704 20:38:49.757589   10176 addons.go:243] addon storage-provisioner should already be in state true
I0704 20:38:49.755870   10176 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0704 20:38:49.758273   10176 host.go:66] Checking if "minikube" exists ...
I0704 20:38:49.758777   10176 host.go:66] Checking if "minikube" exists ...
I0704 20:38:49.776094   10176 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 20:38:49.778246   10176 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0704 20:38:49.785894   10176 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 20:38:49.788124   10176 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 20:38:49.842154   10176 out.go:177] 💡  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0704 20:38:49.843464   10176 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0
I0704 20:38:49.843533   10176 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0704 20:38:49.844585   10176 addons.go:243] addon default-storageclass should already be in state true
I0704 20:38:49.845317   10176 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.0
I0704 20:38:49.845317   10176 host.go:66] Checking if "minikube" exists ...
I0704 20:38:49.846393   10176 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0704 20:38:49.848558   10176 out.go:177]     ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.10.0
I0704 20:38:49.849699   10176 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:38:49.849699   10176 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0704 20:38:49.851130   10176 addons.go:426] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:38:49.851130   10176 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16078 bytes)
I0704 20:38:49.856254   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:49.857962   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:49.859151   10176 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0704 20:38:49.923467   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:49.923467   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:49.931119   10176 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0704 20:38:49.931119   10176 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0704 20:38:49.939142   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0704 20:38:49.991536   10176 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61952 SSHKeyPath:C:\Users\hp\.minikube\machines\minikube\id_rsa Username:docker}
I0704 20:38:50.033795   10176 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0704 20:38:50.080196   10176 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0704 20:38:50.131161   10176 api_server.go:52] waiting for apiserver process to appear ...
I0704 20:38:50.144706   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:50.220881   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:38:50.226171   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:38:50.228425   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:38:50.652663   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:53.178700   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.9578189s)
W0704 20:38:53.178787   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:53.179413   10176 retry.go:31] will retry after 323.387542ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:53.180848   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (2.9524224s)
W0704 20:38:53.180848   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:53.180848   10176 retry.go:31] will retry after 348.751974ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:53.180848   10176 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.5281846s)
I0704 20:38:53.180848   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.9546769s)
W0704 20:38:53.180848   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:53.180848   10176 retry.go:31] will retry after 285.415116ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:53.207560   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:53.502491   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:38:53.516109   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:38:53.555196   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:38:53.650844   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:54.988987   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.4728781s)
W0704 20:38:54.988987   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:54.988987   10176 retry.go:31] will retry after 438.363421ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:54.989633   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.4344367s)
W0704 20:38:54.989633   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:54.989633   10176 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.3387884s)
I0704 20:38:54.989633   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.4864955s)
I0704 20:38:54.989633   10176 retry.go:31] will retry after 419.987412ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0704 20:38:54.989781   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:54.989781   10176 retry.go:31] will retry after 314.836328ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:55.010865   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:55.156819   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:55.324263   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:38:55.432643   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:38:55.444483   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:38:55.651473   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:56.666671   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.3424078s)
W0704 20:38:56.666671   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:56.666671   10176 retry.go:31] will retry after 487.593502ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:56.693266   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.2487838s)
W0704 20:38:56.693266   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:56.693266   10176 retry.go:31] will retry after 332.139495ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:56.693266   10176 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.0417937s)
I0704 20:38:56.693266   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.260623s)
W0704 20:38:56.693266   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:56.693266   10176 retry.go:31] will retry after 594.955813ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:56.710105   10176 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0704 20:38:56.790226   10176 api_server.go:72] duration metric: took 7.034883s to wait for apiserver process to appear ...
I0704 20:38:56.790226   10176 api_server.go:88] waiting for apiserver healthz status ...
I0704 20:38:56.790768   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:56.800187   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:38:57.051195   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:38:57.170596   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:38:57.300173   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:57.302291   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:38:57.322064   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:38:57.805601   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:57.817896   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:38:58.291520   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:58.299719   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:38:58.792778   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:58.796463   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:38:59.286066   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.2348711s)
W0704 20:38:59.286066   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:59.286066   10176 retry.go:31] will retry after 970.509502ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:59.297406   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:59.300234   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:38:59.596461   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (2.2743971s)
W0704 20:38:59.596461   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:59.596461   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.4258656s)
I0704 20:38:59.596461   10176 retry.go:31] will retry after 882.891184ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0704 20:38:59.596992   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:59.596992   10176 retry.go:31] will retry after 1.042885877s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:38:59.802172   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:38:59.804097   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:00.290895   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:00.295035   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:00.301697   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:39:00.501348   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:39:00.656972   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:39:00.797696   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:00.800307   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:01.304072   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:01.305728   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:01.795517   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:01.797615   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:02.081152   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.7794551s)
W0704 20:39:02.081152   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:02.081152   10176 retry.go:31] will retry after 1.336331672s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:02.208487   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.7071387s)
W0704 20:39:02.209008   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:02.209008   10176 retry.go:31] will retry after 1.536387443s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:02.209008   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.5520353s)
W0704 20:39:02.209008   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:02.209008   10176 retry.go:31] will retry after 803.527507ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:02.291565   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:02.294681   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:02.805893   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:02.807471   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:03.038246   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:39:03.292173   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:03.294117   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:03.442229   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:39:03.777555   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:39:03.790858   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:03.793586   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:04.283356   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.2446026s)
W0704 20:39:04.283356   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:04.283356   10176 retry.go:31] will retry after 2.020562927s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:04.291358   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:04.294393   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:04.667598   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.2250641s)
W0704 20:39:04.667598   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:04.667598   10176 retry.go:31] will retry after 1.506647214s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0704 20:39:04.776409   10176 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:04.776409   10176 retry.go:31] will retry after 997.858494ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/ingress-deploy.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0704 20:39:04.795334   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:04.798875   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": EOF
I0704 20:39:05.295435   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:05.805549   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml
I0704 20:39:06.190622   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0704 20:39:06.329393   10176 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0704 20:39:10.307918   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0704 20:39:10.307918   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:15.324742   10176 api_server.go:269] stopped: https://127.0.0.1:61956/healthz: Get "https://127.0.0.1:61956/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0704 20:39:15.324742   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:15.984684   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0704 20:39:15.984684   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0704 20:39:15.985200   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:15.992838   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0704 20:39:15.992838   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0704 20:39:16.298383   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:16.980982   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:16.980982   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:16.980982   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:17.168041   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:17.168041   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:17.296537   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:17.380992   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:17.380992   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:17.798693   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:17.872589   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:17.872589   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:18.293719   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:18.371412   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:18.371412   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:18.798935   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:18.876663   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:18.876663   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:19.291744   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:19.471449   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:19.471449   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:19.803053   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:19.873501   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:19.873501   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:20.299593   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:20.568981   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:20.568981   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:20.790492   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:20.971988   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:20.971988   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:21.294814   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:21.374088   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:21.374088   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:21.795311   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:21.873130   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:21.873130   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:22.293586   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:22.371082   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:22.371082   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:22.790671   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:22.879109   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:22.879109   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:23.290574   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:23.373462   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:23.373462   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:23.798017   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:23.876543   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:23.876543   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:24.296870   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:24.369216   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:24.369216   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:24.792559   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:24.868100   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:24.868100   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:25.298946   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:25.371092   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:25.371092   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:25.800763   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:25.872395   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:25.872395   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:26.304119   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:26.376626   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:26.377137   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:26.802006   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:26.880581   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0704 20:39:26.881126   10176 api_server.go:103] status: https://127.0.0.1:61956/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0704 20:39:27.295054   10176 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61956/healthz ...
I0704 20:39:27.466904   10176 api_server.go:279] https://127.0.0.1:61956/healthz returned 200:
ok
I0704 20:39:27.582199   10176 api_server.go:141] control plane version: v1.30.0
I0704 20:39:27.582199   10176 api_server.go:131] duration metric: took 30.7919726s to wait for apiserver health ...
I0704 20:39:27.583236   10176 system_pods.go:43] waiting for kube-system pods to appear ...
I0704 20:39:27.826071   10176 system_pods.go:59] 8 kube-system pods found
I0704 20:39:27.826071   10176 system_pods.go:61] "coredns-7db6d8ff4d-4fw89" [862a1f5a-039a-438e-b549-c4d683351bb2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0704 20:39:27.826071   10176 system_pods.go:61] "coredns-7db6d8ff4d-6mphs" [71909e90-8d1e-431e-a1cb-cb9d6dea8e48] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0704 20:39:27.826071   10176 system_pods.go:61] "etcd-minikube" [670594ca-6611-4734-8914-2089a341eb23] Running
I0704 20:39:27.826071   10176 system_pods.go:61] "kube-apiserver-minikube" [b70f8619-f287-4767-a614-6f2679cc6df9] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0704 20:39:27.826071   10176 system_pods.go:61] "kube-controller-manager-minikube" [3c08f7cd-d2b3-4749-92c0-906cad8488c6] Running
I0704 20:39:27.826071   10176 system_pods.go:61] "kube-proxy-ftwr7" [79b51343-cea9-48c2-814d-e3a58f704029] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0704 20:39:27.826071   10176 system_pods.go:61] "kube-scheduler-minikube" [bc63e047-ffbc-4f50-ab9f-6e870f0c26d1] Running
I0704 20:39:27.826071   10176 system_pods.go:61] "storage-provisioner" [a341953c-e697-4b87-bb48-1a98751cb978] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0704 20:39:27.826071   10176 system_pods.go:74] duration metric: took 242.8354ms to wait for pod list to return data ...
I0704 20:39:27.826071   10176 kubeadm.go:576] duration metric: took 38.0707284s to wait for: map[apiserver:true system_pods:true]
I0704 20:39:27.826071   10176 node_conditions.go:102] verifying NodePressure condition ...
I0704 20:39:27.876501   10176 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0704 20:39:27.877106   10176 node_conditions.go:123] node cpu capacity is 16
I0704 20:39:27.877610   10176 node_conditions.go:105] duration metric: took 51.5386ms to run NodePressure ...
I0704 20:39:27.877610   10176 start.go:240] waiting for startup goroutines ...
I0704 20:39:36.367745   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/ingress-deploy.yaml: (30.5621957s)
I0704 20:39:36.368879   10176 addons.go:470] Verifying addon ingress=true in "minikube"
I0704 20:39:36.368879   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (30.178257s)
I0704 20:39:36.370825   10176 out.go:177] 🔎  Verifying ingress addon...
I0704 20:39:36.369568   10176 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (30.040175s)
I0704 20:39:36.380109   10176 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0704 20:39:36.773012   10176 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0704 20:39:36.773524   10176 kapi.go:107] duration metric: took 393.9563ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0704 20:39:36.782404   10176 out.go:177] 🌟  Enabled addons: storage-provisioner, ingress, default-storageclass
I0704 20:39:36.783149   10176 addons.go:505] duration metric: took 47.0288545s for enable addons: enabled=[storage-provisioner ingress default-storageclass]
I0704 20:39:36.783981   10176 start.go:245] waiting for cluster config update ...
I0704 20:39:36.783981   10176 start.go:254] writing updated cluster config ...
I0704 20:39:36.805016   10176 ssh_runner.go:195] Run: rm -f paused
I0704 20:39:37.957937   10176 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0704 20:39:37.959967   10176 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 04 15:09:45 minikube cri-dockerd[1280]: time="2024-07-04T15:09:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/646eb9b401a41c689849872d0da463c765f4fd16d18225f40af50417bd39954c/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:45 minikube cri-dockerd[1280]: time="2024-07-04T15:09:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/749b0f80f38f3641a9d347ccb594214b04d54b2eddac29c0a894fae55d4ed3fc/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:45 minikube cri-dockerd[1280]: time="2024-07-04T15:09:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a8155124632f51d9ee1407b7976ba111660c00dbb787279b83b7d14d956a564/resolv.conf as [nameserver 10.96.0.10 search prometheus.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:46 minikube cri-dockerd[1280]: time="2024-07-04T15:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3bebb2adde3287d40829b674f5368fade6e85d255dc9239ef87dc2bef65080f0/resolv.conf as [nameserver 10.96.0.10 search prometheus.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:46 minikube cri-dockerd[1280]: time="2024-07-04T15:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aad94ece25b9776c1f2375c293b5f46083f3dc758eddb153d100f523bcc2574c/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:46 minikube cri-dockerd[1280]: time="2024-07-04T15:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/89a85141eeef7193bff5378c8600dd723f1e2a6bb085207290c9fd0c271c6a77/resolv.conf as [nameserver 10.96.0.10 search tekton-pipelines.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:46 minikube cri-dockerd[1280]: time="2024-07-04T15:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2c1e5f22dab0b3cede88142c720157e8e6ddf27d5c2202a89de991d34a02a22d/resolv.conf as [nameserver 10.96.0.10 search argocd.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:46 minikube cri-dockerd[1280]: time="2024-07-04T15:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/782f01c66fb3c9974941f908d89d485d5963eb73a42a68e4da7daf3cd031fe0b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:46 minikube cri-dockerd[1280]: time="2024-07-04T15:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/24cc63d8428b812219374ff649e0cccdea87a0d562e42b8bbef70ff0b8247f0a/resolv.conf as [nameserver 10.96.0.10 search tekton-pipelines.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 04 15:09:58 minikube dockerd[960]: time="2024-07-04T15:09:58.860471132Z" level=info msg="ignoring event" container=15013de2f9121e80e58a36d1a92ee2845d04268403c3803bb639b2ae15682ee9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:09:59 minikube cri-dockerd[1280]: time="2024-07-04T15:09:59Z" level=info msg="Stop pulling image redis:7.0.14-alpine: Status: Image is up to date for redis:7.0.14-alpine"
Jul 04 15:10:05 minikube cri-dockerd[1280]: time="2024-07-04T15:10:05Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.11.0: Status: Image is up to date for quay.io/argoproj/argocd:v2.11.0"
Jul 04 15:10:11 minikube cri-dockerd[1280]: time="2024-07-04T15:10:11Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.11.0: Status: Image is up to date for quay.io/argoproj/argocd:v2.11.0"
Jul 04 15:10:17 minikube cri-dockerd[1280]: time="2024-07-04T15:10:17Z" level=info msg="Stop pulling image prom/prometheus:latest: Status: Image is up to date for prom/prometheus:latest"
Jul 04 15:10:26 minikube cri-dockerd[1280]: time="2024-07-04T15:10:26Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.11.0: Status: Image is up to date for quay.io/argoproj/argocd:v2.11.0"
Jul 04 15:10:28 minikube dockerd[960]: time="2024-07-04T15:10:28.575038269Z" level=info msg="ignoring event" container=4d4c99066792b80d56808acca659d0616489c1ccc0f01b6f3c0d22343e51ec91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:10:32 minikube cri-dockerd[1280]: time="2024-07-04T15:10:32Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.11.0: Status: Image is up to date for quay.io/argoproj/argocd:v2.11.0"
Jul 04 15:10:35 minikube dockerd[960]: time="2024-07-04T15:10:35.378306828Z" level=info msg="ignoring event" container=05b69da4d7bb9d1cb8affd11f60412d6ca6ee29e42bb4c8d06102f989af95876 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:10:36 minikube cri-dockerd[1280]: time="2024-07-04T15:10:36Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.11.0: Status: Image is up to date for quay.io/argoproj/argocd:v2.11.0"
Jul 04 15:10:42 minikube cri-dockerd[1280]: time="2024-07-04T15:10:42Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.11.0: Status: Image is up to date for quay.io/argoproj/argocd:v2.11.0"
Jul 04 15:10:46 minikube cri-dockerd[1280]: time="2024-07-04T15:10:46Z" level=info msg="Stop pulling image ghcr.io/dexidp/dex:v2.38.0: Status: Image is up to date for ghcr.io/dexidp/dex:v2.38.0"
Jul 04 15:10:51 minikube cri-dockerd[1280]: time="2024-07-04T15:10:51Z" level=info msg="Stop pulling image prom/prometheus:latest: Status: Image is up to date for prom/prometheus:latest"
Jul 04 15:11:03 minikube dockerd[960]: time="2024-07-04T15:11:03.077762760Z" level=info msg="ignoring event" container=05b66715c006b156132c5da0ef39a996807dfd7b816ed1492cbf99fb63526eb2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:11:10 minikube cri-dockerd[1280]: time="2024-07-04T15:11:10Z" level=error msg="Failed to retrieve checkpoint for sandbox 8103b27e5b0f7ea5026854e75eafcb4d6b8ced74e63c980df67f2fc31e8cd92c: checkpoint is not found"
Jul 04 15:12:32 minikube dockerd[960]: time="2024-07-04T15:12:31.846398600Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/prom/prometheus/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)prom%!F(MISSING)prometheus%!A(MISSING)pull&service=registry.docker.io\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" spanID=48dcb1d427fa1a3d traceID=67c1e7bd43a7f3bb09db570a98371eb5
Jul 04 15:13:23 minikube dockerd[960]: time="2024-07-04T15:13:23.040697607Z" level=warning msg="failed to delete container from containerd" container=2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 error="context deadline exceeded"
Jul 04 15:13:57 minikube cri-dockerd[1280]: time="2024-07-04T15:13:56Z" level=error msg="operation timeout: context deadline exceededFailed to get image list from docker"
Jul 04 15:14:06 minikube dockerd[960]: time="2024-07-04T15:14:06.536296654Z" level=error msg="Handler for POST /v1.44/images/create returned error: Head \"https://registry-1.docker.io/v2/prom/prometheus/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)prom%!F(MISSING)prometheus%!A(MISSING)pull&service=registry.docker.io\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" spanID=48dcb1d427fa1a3d traceID=67c1e7bd43a7f3bb09db570a98371eb5
Jul 04 15:14:14 minikube dockerd[960]: time="2024-07-04T15:14:13.448954698Z" level=error msg="Handler for GET /v1.44/images/json returned error: write unix /var/run/docker.sock->@: write: broken pipe" spanID=17df7f63b1c2b7c5 traceID=2148bbe0637a22c883e6f2cf1e47c8db
Jul 04 15:14:17 minikube dockerd[960]: 2024/07/04 15:14:16 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 04 15:14:23 minikube dockerd[960]: time="2024-07-04T15:14:22.734395522Z" level=error msg="2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 cleanup: failed to delete container from containerd: cannot delete running task 2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2: failed precondition"
Jul 04 15:14:52 minikube dockerd[960]: time="2024-07-04T15:14:50.440119284Z" level=error msg="Handler for GET /v1.44/containers/json returned error: write unix /var/run/docker.sock->@: write: broken pipe" spanID=9630368e4bb9831e traceID=02b19632ad30e385b6ba52f92b5af629
Jul 04 15:14:58 minikube dockerd[960]: 2024/07/04 15:14:58 http: superfluous response.WriteHeader call from go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*respWriterWrapper).WriteHeader (wrap.go:98)
Jul 04 15:15:25 minikube dockerd[960]: time="2024-07-04T15:15:24.041116048Z" level=warning msg="failed to delete container from containerd" container=2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 error="context deadline exceeded"
Jul 04 15:16:08 minikube dockerd[960]: time="2024-07-04T15:16:07.327950009Z" level=error msg="2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 cleanup: failed to delete container from containerd: cannot delete running task 2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2: failed precondition"
Jul 04 15:17:03 minikube dockerd[960]: time="2024-07-04T15:17:03.015655956Z" level=warning msg="failed to delete container from containerd" container=2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 error="context deadline exceeded"
Jul 04 15:17:34 minikube dockerd[960]: time="2024-07-04T15:17:34.520206296Z" level=error msg="2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 cleanup: failed to delete container from containerd: cannot delete running task 2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2: failed precondition"
Jul 04 15:18:25 minikube dockerd[960]: time="2024-07-04T15:18:24.900172786Z" level=warning msg="failed to delete container from containerd" container=2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 error="context deadline exceeded"
Jul 04 15:19:01 minikube dockerd[960]: time="2024-07-04T15:19:01.401974171Z" level=error msg="2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 cleanup: failed to delete container from containerd: cannot delete running task 2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2: failed precondition"
Jul 04 15:19:24 minikube cri-dockerd[1280]: time="2024-07-04T15:19:24Z" level=error msg="error getting RW layer size for container ID 'abb06f76effb3e9dd09bf6f0fbb2c3bad2b615e5e1d823c69e25f660a68d5bf4': Error response from daemon: No such container: abb06f76effb3e9dd09bf6f0fbb2c3bad2b615e5e1d823c69e25f660a68d5bf4"
Jul 04 15:19:25 minikube cri-dockerd[1280]: time="2024-07-04T15:19:24Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'abb06f76effb3e9dd09bf6f0fbb2c3bad2b615e5e1d823c69e25f660a68d5bf4'"
Jul 04 15:19:44 minikube dockerd[960]: time="2024-07-04T15:19:44.000113441Z" level=warning msg="failed to delete container from containerd" container=2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 error="context deadline exceeded"
Jul 04 15:19:55 minikube dockerd[960]: time="2024-07-04T15:19:55.376395383Z" level=error msg="2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2 cleanup: failed to delete container from containerd: cannot delete running task 2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2: failed precondition"
Jul 04 15:20:33 minikube dockerd[960]: time="2024-07-04T15:20:33.477983986Z" level=info msg="Container failed to exit within 2s of signal 15 - using the force" container=01b2a7f104f328fc425e1da9d2dc5cf6e82c057eae143aadee357cb814ca130b spanID=7daf256c69704bea traceID=4a863170b74f4bbdfb3e3b7738ba56e3
Jul 04 15:20:33 minikube dockerd[960]: time="2024-07-04T15:20:33.773545220Z" level=info msg="ignoring event" container=c1edaae86d05d04acc006549035788dec079a6240b2a215dbe930242175d02d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:33 minikube dockerd[960]: time="2024-07-04T15:20:33.774421795Z" level=info msg="ignoring event" container=fa3a61399652565838960d0486616f1b76053c3d91494b3a50124388fc1ea6ce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:35 minikube dockerd[960]: time="2024-07-04T15:20:34.283614259Z" level=info msg="ignoring event" container=4137424fcfd770b97b0f8825c47cf7433a8329dd87c703c9c6ff8fc20e6d162b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:35 minikube dockerd[960]: time="2024-07-04T15:20:35.583199531Z" level=info msg="ignoring event" container=932f0a75e817612b03bfbbeabcc6dc83c22478b4e546f59bc44d4220a156834e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:36 minikube dockerd[960]: time="2024-07-04T15:20:36.080827296Z" level=info msg="ignoring event" container=ac9fd0cc297f1459b910078a10d5c887c725629e867dbae9da57eb95d6dbad99 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:36 minikube dockerd[960]: time="2024-07-04T15:20:36.080877711Z" level=info msg="ignoring event" container=31a2c82fe9e7314309f5cc29a5d6edb1ef547d15ab3ad5ee4346be84988f3f20 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:36.673627242Z" level=info msg="ignoring event" container=998b9fc766c671040deb564be0f3259c982837113b7680e4c5a45fa8531457d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:36.779640917Z" level=info msg="ignoring event" container=1903d7cccc4bce47ed4655c2373c617841749319c0f8b3c17976c923b0e9f3f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:37.676083985Z" level=info msg="ignoring event" container=0f3296371ab58ce6f857adb394522f45e6812efb7b3cf3f0232b74df43ae9b55 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:37.773374115Z" level=info msg="ignoring event" container=267622c2176e62c7da9e2d7d3bbb477d1689da653fc4f7218f8f1e8446818242 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:37.774959513Z" level=info msg="ignoring event" container=163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:37.778520312Z" level=info msg="ignoring event" container=547ca744ed8169573023866cc8f081af10f3beae4a85d80f34e501c58a689039 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:37.878376033Z" level=info msg="ignoring event" container=7d902db462f04ebb99f07c98fb3080cc58d41c53254b98015b5c0eecab8374a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:37 minikube dockerd[960]: time="2024-07-04T15:20:37.976106777Z" level=info msg="ignoring event" container=01b2a7f104f328fc425e1da9d2dc5cf6e82c057eae143aadee357cb814ca130b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:38 minikube dockerd[960]: time="2024-07-04T15:20:38.273829945Z" level=info msg="ignoring event" container=2acaa23eabfbb0becad4c9657a0f167d2ceaa42ab4f01d6a77bba3b185ba7762 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 04 15:20:40 minikube cri-dockerd[1280]: time="2024-07-04T15:20:40Z" level=info msg="Stop pulling image prom/prometheus:latest: Status: Image is up to date for prom/prometheus:latest"


==> container status <==
CONTAINER           IMAGE                                                                                             CREATED             STATE               NAME                                 ATTEMPT             POD ID              POD
9cbbb6b27a6b4       b74abbcc4eacb                                                                                     4 seconds ago       Created             prometheus-server                    7                   8560516a6e8ea       prometheus-server-5787759b8c-68cjv
ceace837bb05e       prom/prometheus@sha256:075b1ba2c4ebb04bc3a6ab86c06ec8d8099f8fda1c96ef6d104d9bb1def1d8bc           16 seconds ago      Created             prometheus                           16                  4a8155124632f       prometheus-6f6ddc8745-d4gs4
2669199b98ff7       f3949c8f211e3                                                                                     17 seconds ago      Running             tekton-pipelines-controller          25                  24cc63d8428b8       tekton-pipelines-controller-c5bc687cf-7dqks
c81d2c0f9a6f1       cbb01a7bd410d                                                                                     17 seconds ago      Running             coredns                              20                  6bdf066e71690       coredns-7db6d8ff4d-6mphs
51fa7f97fe107       c7aad43836fa5                                                                                     17 seconds ago      Created             kube-controller-manager              39                  c8a161e348abf       kube-controller-manager-minikube
3f7b57fae9694       259c8277fcbbc                                                                                     17 seconds ago      Created             kube-scheduler                       26                  f96269a648cea       kube-scheduler-minikube
4907505e53bdc       c42c21cd0ebcb                                                                                     17 seconds ago      Running             grafana                              2                   1435691370e10       grafana-867556d6f5-tljgt
f269699299db6       ffcc66479b5ba                                                                                     17 seconds ago      Created             controller                           43                  5ce2e73055740       ingress-nginx-controller-84df5799c-kf49k
56aeddc681c6d       75cb855e9cefb                                                                                     17 seconds ago      Running             pushgateway                          8                   3bebb2adde328       prometheus-prometheus-pushgateway-58cb869bcc-jbp7c
38dd40accc299       11f11916f8cdf                                                                                     18 seconds ago      Running             alertmanager                         11                  260ee283d20a4       prometheus-alertmanager-0
eea27eeac4513       b20e6a3825670                                                                                     18 seconds ago      Running             kube-state-metrics                   13                  4a0e3d9d3dcb4       prometheus-kube-state-metrics-67848d7455-d7dpt
e1e7a64d2aed9       1a7afb41fc885                                                                                     18 seconds ago      Running             tekton-events-controller             28                  107ad76f5e90d       tekton-events-controller-56b58fcf8b-p9xps
feec02414f394       0c6f6c1bdd47c                                                                                     18 seconds ago      Running             node-exporter                        10                  0651485575a93       prometheus-prometheus-node-exporter-k8wg8
1dad6a87dd483       1dffa950475c8                                                                                     18 seconds ago      Running             tekton-dashboard                     38                  89a85141eeef7       tekton-dashboard-65cdfdc6c6-9cnsl
d5aebd2fd2ca6       6e38f40d628db                                                                                     23 seconds ago      Running             storage-provisioner                  84                  0082891e90175       storage-provisioner
05b66715c006b       prom/prometheus@sha256:075b1ba2c4ebb04bc3a6ab86c06ec8d8099f8fda1c96ef6d104d9bb1def1d8bc           10 minutes ago      Exited              prometheus                           15                  4a8155124632f       prometheus-6f6ddc8745-d4gs4
bcd6b72eef1d9       ghcr.io/dexidp/dex@sha256:b1d793440a98d7ecde7fa5dbc8cee1204ef0e8918d9e51ef6201f50d12d55925        10 minutes ago      Running             dex                                  14                  2c1e5f22dab0b       argocd-dex-server-869bdc7dcb-m5mbm
ac9fd0cc297f1       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   10 minutes ago      Exited              argocd-repo-server                   22                  749b0f80f38f3       argocd-repo-server-f965fdfcf-fjjnx
c1edaae86d05d       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   10 minutes ago      Exited              argocd-server                        6                   aad94ece25b97       argocd-server-5bdf56c97f-b9jkt
05b69da4d7bb9       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   10 minutes ago      Exited              copyutil                             11                  2c1e5f22dab0b       argocd-dex-server-869bdc7dcb-m5mbm
2d8aa764a3a01       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   10 minutes ago      Running             argocd-application-controller        15                  eaa30e2ee1208       argocd-application-controller-0
89f9f8c6e4463       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   10 minutes ago      Running             argocd-applicationset-controller     16                  646eb9b401a41       argocd-applicationset-controller-c4fd6dcdb-rw9sn
4137424fcfd77       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   10 minutes ago      Exited              argocd-notifications-controller      21                  e5a38f4a0a765       argocd-notifications-controller-6bbd5dd8d-sv2z6
4815fce8d2e28       redis@sha256:45de526e9fbc1a4b183957ab93a448294181fae10ced9184fc6efe9956ca0ccc                     10 minutes ago      Running             redis                                15                  0d94f762b2635       argocd-redis-79c9bd545b-g2nm9
2041788cc5ece       6e38f40d628db                                                                                     11 minutes ago      Exited              storage-provisioner                  83                  0082891e90175       storage-provisioner
ac9f1bb8909ba       b74abbcc4eacb                                                                                     11 minutes ago      Exited              prometheus-server                    6                   8560516a6e8ea       prometheus-server-5787759b8c-68cjv
998b9fc766c67       b20e6a3825670                                                                                     11 minutes ago      Exited              kube-state-metrics                   12                  4a0e3d9d3dcb4       prometheus-kube-state-metrics-67848d7455-d7dpt
84e78cbc39b4c       9c16a7b3c0844                                                                                     11 minutes ago      Running             rails                                17                  782f01c66fb3c       rails-7556c74fd-v5jx5
15013de2f9121       0046c3e43748c                                                                                     11 minutes ago      Exited              copyutil                             16                  749b0f80f38f3       argocd-repo-server-f965fdfcf-fjjnx
163a9b04452b9       f3949c8f211e3                                                                                     11 minutes ago      Exited              tekton-pipelines-controller          24                  24cc63d8428b8       tekton-pipelines-controller-c5bc687cf-7dqks
1903d7cccc4bc       1dffa950475c8                                                                                     11 minutes ago      Exited              tekton-dashboard                     37                  89a85141eeef7       tekton-dashboard-65cdfdc6c6-9cnsl
0f3296371ab58       75cb855e9cefb                                                                                     11 minutes ago      Exited              pushgateway                          7                   3bebb2adde328       prometheus-prometheus-pushgateway-58cb869bcc-jbp7c
01b2a7f104f32       ffcc66479b5ba                                                                                     11 minutes ago      Exited              controller                           42                  5ce2e73055740       ingress-nginx-controller-84df5799c-kf49k
7d902db462f04       c42c21cd0ebcb                                                                                     11 minutes ago      Exited              grafana                              1                   1435691370e10       grafana-867556d6f5-tljgt
11d9feccde773       1644698813a5c                                                                                     11 minutes ago      Running             webhook                              36                  b43ee47a03776       tekton-pipelines-webhook-644f45c89f-8q5n9
fa3a613996525       11f11916f8cdf                                                                                     11 minutes ago      Exited              alertmanager                         10                  260ee283d20a4       prometheus-alertmanager-0
c245ae008de3f       ebf01b748a56f                                                                                     11 minutes ago      Running             postgres                             13                  727a1677283e7       postgres-0
2acaa23eabfbb       cbb01a7bd410d                                                                                     11 minutes ago      Exited              coredns                              19                  6bdf066e71690       coredns-7db6d8ff4d-6mphs
599805a4ec5fa       c7b6ac19f8751                                                                                     11 minutes ago      Running             prometheus-server-configmap-reload   3                   8560516a6e8ea       prometheus-server-5787759b8c-68cjv
fffcb60ddcd25       9c16a7b3c0844                                                                                     11 minutes ago      Running             rails                                13                  0c11ce82db73b       rails-7556c74fd-7qmkk
67571596e95dd       cbb01a7bd410d                                                                                     11 minutes ago      Running             coredns                              20                  085013d021f27       coredns-7db6d8ff4d-4fw89
9e02229a6b094       9b870f148a4eb                                                                                     11 minutes ago      Running             controller                           13                  021ab146d18f6       tekton-pipelines-remote-resolvers-659495bc6-9kpvm
31a2c82fe9e73       1a7afb41fc885                                                                                     11 minutes ago      Exited              tekton-events-controller             27                  107ad76f5e90d       tekton-events-controller-56b58fcf8b-p9xps
37e2c62345918       ebf01b748a56f                                                                                     11 minutes ago      Running             postgres                             17                  e194bceacb31f       postgres-0
547ca744ed816       c7aad43836fa5                                                                                     11 minutes ago      Exited              kube-controller-manager              38                  c8a161e348abf       kube-controller-manager-minikube
e1c0a8e431cbf       a0bf559e280cf                                                                                     11 minutes ago      Running             kube-proxy                           18                  7fd359106796e       kube-proxy-ftwr7
932f0a75e8176       0c6f6c1bdd47c                                                                                     11 minutes ago      Exited              node-exporter                        9                   0651485575a93       prometheus-prometheus-node-exporter-k8wg8
deaa802192db3       c42f13656d0b2                                                                                     12 minutes ago      Running             kube-apiserver                       24                  a122e27591946       kube-apiserver-minikube
c5a7d23c3462c       3861cfcd7c04c                                                                                     12 minutes ago      Running             etcd                                 18                  c3c4cb6d64cca       etcd-minikube
267622c2176e6       259c8277fcbbc                                                                                     12 minutes ago      Exited              kube-scheduler                       25                  f96269a648cea       kube-scheduler-minikube
3bf95a2bd6626       c7aad43836fa5                                                                                     12 minutes ago      Exited              kube-controller-manager              37                  c8a161e348abf       kube-controller-manager-minikube
271237b23271a       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   28 minutes ago      Exited              argocd-applicationset-controller     15                  fcc548f20a37b       argocd-applicationset-controller-c4fd6dcdb-rw9sn
1cd5b7d539e46       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   28 minutes ago      Exited              argocd-server                        5                   ebe12105b3cbb       argocd-server-5bdf56c97f-b9jkt
c70ce6f17c3e5       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   28 minutes ago      Exited              argocd-notifications-controller      20                  85673fd9d563f       argocd-notifications-controller-6bbd5dd8d-sv2z6
e8cc32d30e992       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   28 minutes ago      Exited              argocd-application-controller        14                  c0566b01ed70b       argocd-application-controller-0
427738c7e8659       redis@sha256:45de526e9fbc1a4b183957ab93a448294181fae10ced9184fc6efe9956ca0ccc                     28 minutes ago      Exited              redis                                14                  0306f20a1d518       argocd-redis-79c9bd545b-g2nm9
a3e9883102923       grafana/grafana@sha256:079600c9517b678c10cda6006b4487d3174512fd4c6cface37df7822756ed7a5           28 minutes ago      Exited              grafana                              0                   0910e5586a3eb       grafana-867556d6f5-tljgt
9f1989093f029       b74abbcc4eacb                                                                                     30 minutes ago      Exited              prometheus-server                    5                   173842f6762ad       prometheus-server-5787759b8c-68cjv
26daab3f309a5       ffcc66479b5ba                                                                                     30 minutes ago      Exited              controller                           41                  1bfee7074d34c       ingress-nginx-controller-84df5799c-kf49k
2cb08651ac2b0       ebf01b748a56f                                                                                     30 minutes ago      Exited              postgres                             16                  fc9a40f5ac414       postgres-0
e3dbf2e4359ee       11f11916f8cdf                                                                                     30 minutes ago      Exited              alertmanager                         9                   4c76d00363711       prometheus-alertmanager-0
abfba91a7b40f       c7b6ac19f8751                                                                                     30 minutes ago      Exited              prometheus-server-configmap-reload   2                   173842f6762ad       prometheus-server-5787759b8c-68cjv
7a72aeafb5e2e       75cb855e9cefb                                                                                     30 minutes ago      Exited              pushgateway                          6                   627f2c1147c32       prometheus-prometheus-pushgateway-58cb869bcc-jbp7c
89f1178b8b0ce       9c16a7b3c0844                                                                                     30 minutes ago      Exited              rails                                12                  d6e36a896af96       rails-7556c74fd-7qmkk
a5d3c6f6e5607       cbb01a7bd410d                                                                                     30 minutes ago      Exited              coredns                              19                  6ac6aaa709959       coredns-7db6d8ff4d-4fw89
00302076b0b49       1a7afb41fc885                                                                                     30 minutes ago      Exited              tekton-events-controller             26                  6749d7f543784       tekton-events-controller-56b58fcf8b-p9xps
17e059576f1ec       9c16a7b3c0844                                                                                     30 minutes ago      Exited              rails                                16                  2ec29d2d8c6bc       rails-7556c74fd-v5jx5
d48a33baedd74       cbb01a7bd410d                                                                                     30 minutes ago      Exited              coredns                              18                  1cdbd82fb3e8b       coredns-7db6d8ff4d-6mphs
e91ee36dd3bbb       1dffa950475c8                                                                                     30 minutes ago      Exited              tekton-dashboard                     36                  a1706a192897f       tekton-dashboard-65cdfdc6c6-9cnsl
05b3094afb13e       ebf01b748a56f                                                                                     30 minutes ago      Exited              postgres                             12                  ad76e2db684fa       postgres-0
a3e1461cff256       1644698813a5c                                                                                     30 minutes ago      Exited              webhook                              35                  bf431bc7f1bb4       tekton-pipelines-webhook-644f45c89f-8q5n9
4ceb1b562819f       f3949c8f211e3                                                                                     30 minutes ago      Exited              tekton-pipelines-controller          23                  c98f874632ecd       tekton-pipelines-controller-c5bc687cf-7dqks
d654908888c52       9b870f148a4eb                                                                                     30 minutes ago      Exited              controller                           12                  035e9a92d6739       tekton-pipelines-remote-resolvers-659495bc6-9kpvm
053dde9ce66f0       b20e6a3825670                                                                                     30 minutes ago      Exited              kube-state-metrics                   11                  48e5266ac9614       prometheus-kube-state-metrics-67848d7455-d7dpt
b433a037a1f8b       a0bf559e280cf                                                                                     31 minutes ago      Exited              kube-proxy                           17                  db287c0a7149d       kube-proxy-ftwr7
0a1c33498347c       0c6f6c1bdd47c                                                                                     31 minutes ago      Exited              node-exporter                        8                   8f4cc58dcc242       prometheus-prometheus-node-exporter-k8wg8
61179c7bfbc71       c42f13656d0b2                                                                                     31 minutes ago      Exited              kube-apiserver                       23                  3525e9bcd0c18       kube-apiserver-minikube
d892d4f498c3b       259c8277fcbbc                                                                                     31 minutes ago      Exited              kube-scheduler                       24                  03365c7373090       kube-scheduler-minikube
58c30bc3b50bb       3861cfcd7c04c                                                                                     31 minutes ago      Exited              etcd                                 17                  8cef8c88510da       etcd-minikube
ce67a02024fd0       ghcr.io/dexidp/dex@sha256:b1d793440a98d7ecde7fa5dbc8cee1204ef0e8918d9e51ef6201f50d12d55925        3 hours ago         Exited              dex                                  13                  1fa77b2b2eb34       argocd-dex-server-869bdc7dcb-m5mbm
af92fd1518af8       quay.io/argoproj/argocd@sha256:e81cfc1f5761edc54684e00ca7f368a2a95ccd80a032ad9f1f97eaa3fdfb06c7   3 hours ago         Unknown             argocd-repo-server                   21                  d6aef2e9d15c0       argocd-repo-server-f965fdfcf-fjjnx


==> controller_ingress [01b2a7f104f3] <==
W0704 15:10:05.032113       6 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0704 15:10:05.063063       6 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0704 15:10:05.374896       6 main.go:249] "Running in Kubernetes cluster" major="1" minor="30" git="v1.30.0" state="clean" commit="7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a" platform="linux/amd64"
I0704 15:10:08.377094       6 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0704 15:10:09.562236       6 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0704 15:10:09.966226       6 nginx.go:265] "Starting NGINX Ingress controller"
I0704 15:10:10.971506       6 event.go:364] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"305a8d20-4df2-49e3-9e6b-8577145d0b61", APIVersion:"v1", ResourceVersion:"13415", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0704 15:10:10.978311       6 event.go:364] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"2d772b0c-9943-4c11-96de-6bd0f8db2cc8", APIVersion:"v1", ResourceVersion:"11108", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0704 15:10:10.978407       6 event.go:364] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"eae40eef-3bff-4bf1-8ea3-60d7df0aea15", APIVersion:"v1", ResourceVersion:"11109", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0704 15:10:12.076160       6 store.go:440] "Found valid IngressClass" ingress="default/rails-ingress" ingressclass="nginx"
I0704 15:10:12.077137       6 event.go:364] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"rails-ingress", UID:"98ed6bd4-4b0d-4f48-bc9b-d00047a621d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"201526", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0704 15:10:12.169127       6 nginx.go:308] "Starting NGINX process"
I0704 15:10:12.169500       6 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0704 15:10:12.181275       6 nginx.go:328] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0704 15:10:12.494449       6 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0704 15:10:12.494638       6 status.go:84] "New leader elected" identity="ingress-nginx-controller-84df5799c-kf49k"
W0704 15:10:12.495568       6 controller.go:1214] Service "default/rails" does not have any active Endpoint.
I0704 15:10:12.496404       6 controller.go:190] "Configuration changes detected, backend reload required"
I0704 15:10:12.560479       6 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-84df5799c-kf49k" node="minikube"
I0704 15:10:12.579301       6 status.go:304] "updating Ingress status" namespace="default" ingress="rails-ingress" currentValue=[{"ip":"192.168.58.2"}] newValue=[]
I0704 15:10:12.600591       6 event.go:364] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"rails-ingress", UID:"98ed6bd4-4b0d-4f48-bc9b-d00047a621d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"202147", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0704 15:10:13.275816       6 controller.go:210] "Backend successfully reloaded"
I0704 15:10:13.276014       6 controller.go:221] "Initial sync, sleeping for 1 second"
I0704 15:10:13.276069       6 event.go:364] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-84df5799c-kf49k", UID:"d2451fc1-7eb4-4f48-bcc4-4ca38dfbea63", APIVersion:"v1", ResourceVersion:"202067", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0704 15:10:22.857441       6 controller.go:1214] Service "default/rails" does not have any active Endpoint.
I0704 15:11:12.672563       6 status.go:304] "updating Ingress status" namespace="default" ingress="rails-ingress" currentValue=null newValue=[{"ip":"192.168.58.2"}]
I0704 15:11:12.715055       6 event.go:364] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"rails-ingress", UID:"98ed6bd4-4b0d-4f48-bc9b-d00047a621d1", APIVersion:"networking.k8s.io/v1", ResourceVersion:"202398", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
E0704 15:11:54.547143       6 leaderelection.go:332] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0704 15:11:54.952152       6 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0704 15:11:59.049331       6 status.go:104] "error running poll" err="timed out waiting for the condition"
I0704 15:11:58.854638       6 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
E0704 15:14:34.933427       6 leaderelection.go:369] Failed to update lock: the server was unable to return a response in the time allotted, but may still be processing the request (put leases.coordination.k8s.io ingress-nginx-leader)
I0704 15:16:04.729625       6 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
E0704 15:16:21.530338       6 leaderelection.go:369] Failed to update lock: Put "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0704 15:16:21.829791       6 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0704 15:16:22.434160       6 status.go:104] "error running poll" err="timed out waiting for the condition"
I0704 15:16:22.435166       6 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
E0704 15:17:48.702220       6 leaderelection.go:369] Failed to update lock: Timeout: request did not complete within requested timeout - context deadline exceeded
I0704 15:19:14.099642       6 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
E0704 15:19:31.686811       6 leaderelection.go:369] Failed to update lock: client rate limiter Wait returned an error: context deadline exceeded
I0704 15:19:31.884320       6 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
E0704 15:19:33.092584       6 status.go:104] "error running poll" err="timed out waiting for the condition"
I0704 15:19:33.086682       6 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0704 15:20:29.571902       6 sigterm.go:36] "Received SIGTERM, shutting down"
I0704 15:20:29.971885       6 nginx.go:384] "Shutting down controller queues"
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.10.0
  Build:         71f78d49f0a496c31d4c19f095469f3f23900f8a
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.3

-------------------------------------------------------------------------------



==> controller_ingress [26daab3f309a] <==
E0704 14:56:33.862243       7 reflector.go:147] k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: Failed to watch *v1.IngressClass: failed to list *v1.IngressClass: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=201713": net/http: TLS handshake timeout
E0704 14:56:33.957089       7 reflector.go:147] k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: Failed to watch *v1.Ingress: failed to list *v1.Ingress: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingresses?resourceVersion=201716": net/http: TLS handshake timeout
W0704 14:56:49.853391       7 reflector.go:539] k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201697": net/http: TLS handshake timeout
E0704 14:56:49.752143       7 leaderelection.go:332] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": net/http: TLS handshake timeout
I0704 14:56:51.456420       7 trace.go:236] Trace[1465738263]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:56:22.355) (total time: 28601ms):
Trace[1465738263]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201697": net/http: TLS handshake timeout 26899ms (14:56:49.252)
Trace[1465738263]: [28.601737713s] [28.601737713s] END
E0704 14:56:53.161656       7 reflector.go:147] k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201697": net/http: TLS handshake timeout
W0704 14:57:22.450918       7 reflector.go:539] k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: failed to list *v1.IngressClass: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=201713": net/http: TLS handshake timeout
E0704 14:57:22.649182       7 leaderelection.go:332] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": net/http: TLS handshake timeout
I0704 14:57:22.748775       7 trace.go:236] Trace[1304349295]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:57:02.951) (total time: 19701ms):
Trace[1304349295]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=201713": net/http: TLS handshake timeout 19310ms (14:57:22.257)
Trace[1304349295]: [19.701061018s] [19.701061018s] END
E0704 14:57:23.047792       7 reflector.go:147] k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: Failed to watch *v1.IngressClass: failed to list *v1.IngressClass: Get "https://10.96.0.1:443/apis/networking.k8s.io/v1/ingressclasses?resourceVersion=201713": net/http: TLS handshake timeout
I0704 14:57:46.352638       7 trace.go:236] Trace[1638295566]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:57:16.553) (total time: 29500ms):
Trace[1638295566]: ---"Objects listed" error:<nil> 28995ms (14:57:45.545)
Trace[1638295566]: [29.500474013s] [29.500474013s] END
I0704 14:57:46.250273       7 trace.go:236] Trace[1586494487]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:57:14.754) (total time: 31394ms):
Trace[1586494487]: ---"Objects listed" error:<nil> 30796ms (14:57:45.546)
Trace[1586494487]: [31.394310108s] [31.394310108s] END
I0704 14:57:50.347340       7 trace.go:236] Trace[2098330002]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:57:12.148) (total time: 37704ms):
Trace[2098330002]: ---"Objects listed" error:<nil> 37299ms (14:57:49.443)
Trace[2098330002]: [37.704723997s] [37.704723997s] END
I0704 14:57:50.443088       7 trace.go:236] Trace[637735880]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:57:23.651) (total time: 25699ms):
Trace[637735880]: ---"Objects listed" error:<nil> 24903ms (14:57:48.550)
Trace[637735880]: [25.699041624s] [25.699041624s] END
I0704 14:57:50.347539       7 trace.go:236] Trace[593386313]: "DeltaFIFO Pop Process" ID:argocd/argocd-cm,Depth:57,Reason:slow event handlers blocking the queue (04-Jul-2024 14:57:49.848) (total time: 208ms):
Trace[593386313]: [208.32972ms] [208.32972ms] END
I0704 14:58:02.244608       7 trace.go:236] Trace[2013556021]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:57:39.153) (total time: 22498ms):
Trace[2013556021]: ---"Objects listed" error:<nil> 21901ms (14:58:01.050)
Trace[2013556021]: [22.49836103s] [22.49836103s] END
I0704 14:58:50.248093       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0704 14:58:50.437429       7 trace.go:236] Trace[25296812]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229 (04-Jul-2024 14:58:18.844) (total time: 30995ms):
Trace[25296812]: ---"Objects listed" error:<nil> 30794ms (14:58:49.635)
Trace[25296812]: [30.995710943s] [30.995710943s] END
E0704 14:59:11.741415       7 leaderelection.go:369] Failed to update lock: client rate limiter Wait returned an error: context deadline exceeded
I0704 14:59:12.529321       7 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
I0704 14:59:16.834992       7 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
E0704 14:59:16.837543       7 status.go:104] "error running poll" err="timed out waiting for the condition"
E0704 15:00:30.033647       7 leaderelection.go:369] Failed to update lock: etcdserver: request timed out
I0704 15:01:40.412290       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
E0704 15:01:57.720458       7 leaderelection.go:369] Failed to update lock: client rate limiter Wait returned an error: context deadline exceeded
I0704 15:01:57.910944       7 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
I0704 15:01:58.516477       7 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
E0704 15:01:58.522310       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0704 15:02:55.111602       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
E0704 15:03:14.091155       7 leaderelection.go:332] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": context deadline exceeded
I0704 15:03:14.393577       7 leaderelection.go:285] failed to renew lease ingress-nginx/ingress-nginx-leader: timed out waiting for the condition
I0704 15:03:17.401962       7 leaderelection.go:250] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
E0704 15:03:17.495916       7 status.go:104] "error running poll" err="timed out waiting for the condition"
I0704 15:04:26.583678       7 leaderelection.go:260] successfully acquired lease ingress-nginx/ingress-nginx-leader
W0704 15:04:35.182138       7 controller.go:1214] Service "default/rails" does not have any active Endpoint.
I0704 15:04:35.484852       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0704 15:04:35.489267       7 nginx.go:384] "Shutting down controller queues"
I0704 15:04:38.087820       7 status.go:135] "removing value from ingress status" address=[{"ip":"192.168.58.2"}]
I0704 15:04:38.288041       7 nginx.go:392] "Stopping admission controller"
E0704 15:04:38.288173       7 nginx.go:331] "Error listening for TLS connections" err="http: Server closed"
I0704 15:04:38.288181       7 nginx.go:400] "Stopping NGINX process"
E0704 15:04:39.680966       7 leaderelection.go:332] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": dial tcp 10.96.0.1:443: connect: connection refused
2024/07/04 15:04:40 [notice] 571#571: signal process started


==> coredns [2acaa23eabfb] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.499889013s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.799356164s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.396633316s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.505321137s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.506113949s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.799838864s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.903540452s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.198981536s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.30067737s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.099222854s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.201655182s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.104340174s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.804979705s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.407216943s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.893729972s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [67571596e95d] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 591cf328cccc12bc490481273e738df59329c62c0b729d94e8b61db9961c2fa5f046dd37f1cf888b953814040d180f52594972691cd6ff41be96639138a43908
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.311319433s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.798599818s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.402708873s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.505467968s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.594770806s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.799799001s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.401833124s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.300528623s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.000855218s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.201575625s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.198510743s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.802169591s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.005086124s


==> coredns [a5d3c6f6e560] <==
[INFO] plugin/kubernetes: Trace[679761514]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:55:26.473) (total time: 19697ms):
Trace[679761514]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout 18304ms (14:55:44.773)
Trace[679761514]: [19.697892824s] [19.697892824s] END
[INFO] plugin/kubernetes: Trace[150272484]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:55:24.067) (total time: 22098ms):
Trace[150272484]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout 20598ms (14:55:44.661)
Trace[150272484]: [22.09887511s] [22.09887511s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1439343721]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:55:55.061) (total time: 14508ms):
Trace[1439343721]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout 13700ms (14:56:08.761)
Trace[1439343721]: [14.508459699s] [14.508459699s] END
[INFO] plugin/kubernetes: Trace[647990092]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:55:57.561) (total time: 12008ms):
Trace[647990092]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout 11299ms (14:56:08.861)
Trace[647990092]: [12.008444963s] [12.008444963s] END
[INFO] plugin/kubernetes: Trace[1354661399]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:55:54.372) (total time: 15288ms):
Trace[1354661399]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout 14000ms (14:56:08.373)
Trace[1354661399]: [15.288396275s] [15.288396275s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1753278466]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:56:23.463) (total time: 15993ms):
Trace[1753278466]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout 14494ms (14:56:37.958)
Trace[1753278466]: [15.993285311s] [15.993285311s] END
[INFO] plugin/kubernetes: Trace[2135101995]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:56:27.156) (total time: 12401ms):
Trace[2135101995]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout 10699ms (14:56:37.855)
Trace[2135101995]: [12.401527424s] [12.401527424s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?resourceVersion=201722": net/http: TLS handshake timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.090334155s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1473029720]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:56:32.655) (total time: 23507ms):
Trace[1473029720]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout 22709ms (14:56:55.361)
Trace[1473029720]: [23.507465575s] [23.507465575s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?resourceVersion=201680": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[953360050]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:57:09.349) (total time: 14908ms):
Trace[953360050]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout 13603ms (14:57:22.953)
Trace[953360050]: [14.908454748s] [14.908454748s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?resourceVersion=201688": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1718246973]: "DeltaFIFO Pop Process" ID:argocd/argocd-applicationset-controller-slhx5,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 14:57:41.146) (total time: 404ms):
Trace[1718246973]: [404.279564ms] [404.279564ms] END
[INFO] plugin/kubernetes: Trace[1062213005]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:57:19.357) (total time: 21790ms):
Trace[1062213005]: ---"Objects listed" error:<nil> 20994ms (14:57:40.347)
Trace[1062213005]: [21.790263165s] [21.790263165s] END
[INFO] plugin/kubernetes: Trace[1625301203]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:57:48.852) (total time: 10801ms):
Trace[1625301203]: ---"Objects listed" error:<nil> 10697ms (14:57:59.549)
Trace[1625301203]: [10.801799558s] [10.801799558s] END
[INFO] plugin/kubernetes: Trace[1031237725]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (04-Jul-2024 14:58:07.849) (total time: 22299ms):
Trace[1031237725]: ---"Objects listed" error:<nil> 21606ms (14:58:29.451)
Trace[1031237725]: [22.299592956s] [22.299592956s] END
[INFO] plugin/kubernetes: Trace[1124377887]: "DeltaFIFO Pop Process" ID:argocd/argocd-applicationset-controller,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 14:58:30.148) (total time: 598ms):
Trace[1124377887]: [598.158112ms] [598.158112ms] END
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [d48a33baedd7] <==
command /bin/bash -c "docker logs --tail 60 d48a33baedd7" failed with error: /bin/bash -c "docker logs --tail 60 d48a33baedd7": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal


==> describe nodes <==
command /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" failed with error: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?


==> dmesg <==
[  +0.000001] Total swap = 1048576kB
[  +0.000002] 1005727 pages RAM
[  +0.000000] 0 pages HighMem/MovableOnly
[  +0.000001] 28021 pages reserved
[Jul 4 14:53] hrtimer: interrupt took 15885540 ns
[Jul 4 15:07] Exception: 
[  +0.000005] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +0.188240] Exception: 
[  +0.000004] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +1.858828] FS-Cache: Duplicate cookie detected
[  +0.002564] FS-Cache: O-cookie c=000000f2 [p=00000002 fl=222 nc=0 na=1]
[  +0.000603] FS-Cache: O-cookie d=000000008abe104b{9P.session} n=00000000dd62f752
[  +0.001075] FS-Cache: O-key=[10] '34323935303832373130'
[  +0.000399] FS-Cache: N-cookie c=000000f3 [p=00000002 fl=2 nc=0 na=1]
[  +0.000510] FS-Cache: N-cookie d=000000008abe104b{9P.session} n=00000000dac04e71
[  +0.000746] FS-Cache: N-key=[10] '34323935303832373130'
[  +0.036486] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.017532] FS-Cache: Duplicate cookie detected
[  +0.000739] FS-Cache: O-cookie c=000000f5 [p=00000002 fl=222 nc=0 na=1]
[  +0.000474] FS-Cache: O-cookie d=000000008abe104b{9P.session} n=00000000eb98ab62
[  +0.000481] FS-Cache: O-key=[10] '34323935303832373136'
[  +0.000616] FS-Cache: N-cookie c=000000f6 [p=00000002 fl=2 nc=0 na=1]
[  +0.000339] FS-Cache: N-cookie d=000000008abe104b{9P.session} n=0000000043128734
[  +0.001130] FS-Cache: N-key=[10] '34323935303832373136'
[  +0.029666] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.263500] FS-Cache: Duplicate cookie detected
[  +0.000922] FS-Cache: O-cookie c=000000fa [p=00000002 fl=222 nc=0 na=1]
[  +0.000983] FS-Cache: O-cookie d=000000008abe104b{9P.session} n=000000004142f382
[  +0.001145] FS-Cache: O-key=[10] '34323935303832373436'
[  +0.000571] FS-Cache: N-cookie c=000000fc [p=00000002 fl=2 nc=0 na=1]
[  +0.001047] FS-Cache: N-cookie d=000000008abe104b{9P.session} n=0000000069528c87
[  +0.001245] FS-Cache: N-key=[10] '34323935303832373436'
[  +0.012630] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001446] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001157] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.001365] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.005519] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001643] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.003247] WSL (4) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001471] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.002815] WSL (5) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001459] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.008349] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.158114] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000017] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001321] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000956] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001043] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001000] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001224] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000478] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.426921] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [58c30bc3b50b] <==
{"level":"warn","ts":"2024-07-04T15:04:32.29085Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.761432ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528478847970079 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:2cf1907e38b82b1e>","response":"size:41"}
{"level":"info","ts":"2024-07-04T15:04:32.684854Z","caller":"traceutil/trace.go:171","msg":"trace[664943746] transaction","detail":"{read_only:false; response_revision:201844; number_of_response:1; }","duration":"290.49357ms","start":"2024-07-04T15:04:32.394247Z","end":"2024-07-04T15:04:32.68474Z","steps":["trace[664943746] 'process raft request'  (duration: 289.553229ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:34.384333Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238528478847970081,"retry-timeout":"500ms"}
{"level":"info","ts":"2024-07-04T15:04:34.583429Z","caller":"traceutil/trace.go:171","msg":"trace[946298727] linearizableReadLoop","detail":"{readStateIndex:224577; appliedIndex:224577; }","duration":"793.620303ms","start":"2024-07-04T15:04:33.789752Z","end":"2024-07-04T15:04:34.583372Z","steps":["trace[946298727] 'read index received'  (duration: 793.610805ms)","trace[946298727] 'applied index is now lower than readState.Index'  (duration: 7.355µs)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:34.194113Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.809043116s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.taskrun.reconciler.00-of-01\" ","response":"","error":"context canceled"}
{"level":"info","ts":"2024-07-04T15:04:34.583822Z","caller":"traceutil/trace.go:171","msg":"trace[1165810602] range","detail":"{range_begin:/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.taskrun.reconciler.00-of-01; range_end:; }","duration":"2.198915027s","start":"2024-07-04T15:04:32.384866Z","end":"2024-07-04T15:04:34.583781Z","steps":["trace[1165810602] 'agreement among raft nodes before linearized reading'  (duration: 1.809027141s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:34.586248Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.300364627s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.resolutionrequest.reconciler.00-of-01\" ","response":"range_response_count:1 size:715"}
{"level":"info","ts":"2024-07-04T15:04:34.586409Z","caller":"traceutil/trace.go:171","msg":"trace[1557403384] range","detail":"{range_begin:/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.resolutionrequest.reconciler.00-of-01; range_end:; response_count:1; response_revision:201844; }","duration":"2.300562239s","start":"2024-07-04T15:04:32.285782Z","end":"2024-07-04T15:04:34.586344Z","steps":["trace[1557403384] 'agreement among raft nodes before linearized reading'  (duration: 2.297928003s)"],"step_count":1}
{"level":"info","ts":"2024-07-04T15:04:34.590562Z","caller":"traceutil/trace.go:171","msg":"trace[1664365879] transaction","detail":"{read_only:false; response_revision:201845; number_of_response:1; }","duration":"694.90995ms","start":"2024-07-04T15:04:33.895575Z","end":"2024-07-04T15:04:34.590485Z","steps":["trace[1664365879] 'process raft request'  (duration: 691.002421ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:34.586487Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:32.28575Z","time spent":"2.300710698s","remote":"127.0.0.1:51094","response type":"/etcdserverpb.KV/Range","request count":0,"request size":146,"response count":1,"response size":738,"request content":"key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.resolutionrequest.reconciler.00-of-01\" "}
{"level":"warn","ts":"2024-07-04T15:04:34.682152Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:32.384834Z","time spent":"2.29723772s","remote":"127.0.0.1:51094","response type":"/etcdserverpb.KV/Range","request count":0,"request size":136,"response count":0,"response size":0,"request content":"key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.taskrun.reconciler.00-of-01\" "}
{"level":"warn","ts":"2024-07-04T15:04:34.782475Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:33.895517Z","time spent":"695.135325ms","remote":"127.0.0.1:51094","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":604,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.pipelinerun.reconciler.00-of-01\" mod_revision:201844 > success:<request_put:<key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.pipelinerun.reconciler.00-of-01\" value_size:458 >> failure:<request_range:<key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.pipelinerun.reconciler.00-of-01\" > >"}
2024/07/04 15:04:35 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-07-04T15:04:35.688698Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"604.997064ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528478847970084 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/deployments/prometheus/prometheus-server\" mod_revision:201468 > success:<request_put:<key:\"/registry/deployments/prometheus/prometheus-server\" value_size:5344 >> failure:<request_range:<key:\"/registry/deployments/prometheus/prometheus-server\" > >>","response":"size:18"}
{"level":"info","ts":"2024-07-04T15:04:35.689165Z","caller":"traceutil/trace.go:171","msg":"trace[1709081615] transaction","detail":"{read_only:false; response_revision:201846; number_of_response:1; }","duration":"1.704774458s","start":"2024-07-04T15:04:33.984352Z","end":"2024-07-04T15:04:35.689127Z","steps":["trace[1709081615] 'process raft request'  (duration: 1.098565555s)","trace[1709081615] 'compare'  (duration: 603.433203ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:35.689415Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:33.984295Z","time spent":"1.70500821s","remote":"127.0.0.1:51282","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":5402,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/deployments/prometheus/prometheus-server\" mod_revision:201468 > success:<request_put:<key:\"/registry/deployments/prometheus/prometheus-server\" value_size:5344 >> failure:<request_range:<key:\"/registry/deployments/prometheus/prometheus-server\" > >"}
{"level":"info","ts":"2024-07-04T15:04:35.783674Z","caller":"traceutil/trace.go:171","msg":"trace[752978545] transaction","detail":"{read_only:false; response_revision:201847; number_of_response:1; }","duration":"1.394783261s","start":"2024-07-04T15:04:34.388841Z","end":"2024-07-04T15:04:35.783624Z","steps":["trace[752978545] 'process raft request'  (duration: 1.300080191s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:35.783917Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:34.388797Z","time spent":"1.395020335s","remote":"127.0.0.1:46926","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:0 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238528478847970078 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >"}
{"level":"info","ts":"2024-07-04T15:04:35.785996Z","caller":"traceutil/trace.go:171","msg":"trace[2143329297] transaction","detail":"{read_only:false; response_revision:201848; number_of_response:1; }","duration":"1.3966521s","start":"2024-07-04T15:04:34.389247Z","end":"2024-07-04T15:04:35.785899Z","steps":["trace[2143329297] 'process raft request'  (duration: 1.393155334s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:35.786215Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:34.389215Z","time spent":"1.396889922s","remote":"127.0.0.1:50890","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":744,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/events/tekton-pipelines/tekton-events-controller-56b58fcf8b-p9xps.17df0ad038051017\" mod_revision:0 > success:<request_put:<key:\"/registry/events/tekton-pipelines/tekton-events-controller-56b58fcf8b-p9xps.17df0ad038051017\" value_size:634 lease:3238528478847970020 >> failure:<>"}
{"level":"info","ts":"2024-07-04T15:04:35.786553Z","caller":"traceutil/trace.go:171","msg":"trace[800669643] transaction","detail":"{read_only:false; response_revision:201849; number_of_response:1; }","duration":"1.204207201s","start":"2024-07-04T15:04:34.58231Z","end":"2024-07-04T15:04:35.786517Z","steps":["trace[800669643] 'process raft request'  (duration: 1.202648598s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:35.786761Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:34.582285Z","time spent":"1.204389848s","remote":"127.0.0.1:51094","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":596,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.taskrun.reconciler.00-of-01\" mod_revision:201843 > success:<request_put:<key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.taskrun.reconciler.00-of-01\" value_size:454 >> failure:<request_range:<key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.taskrun.reconciler.00-of-01\" > >"}
{"level":"warn","ts":"2024-07-04T15:04:36.184842Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"399.678325ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528478847970089 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/deployments/default/rails\" mod_revision:198464 > success:<request_put:<key:\"/registry/deployments/default/rails\" value_size:3109 >> failure:<request_range:<key:\"/registry/deployments/default/rails\" > >>","response":"size:18"}
{"level":"info","ts":"2024-07-04T15:04:36.185039Z","caller":"traceutil/trace.go:171","msg":"trace[2122339012] transaction","detail":"{read_only:false; response_revision:201850; number_of_response:1; }","duration":"1.501860762s","start":"2024-07-04T15:04:34.683159Z","end":"2024-07-04T15:04:36.18502Z","steps":["trace[2122339012] 'process raft request'  (duration: 1.101937242s)","trace[2122339012] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/deployments/default/rails; req_size:3149; } (duration: 394.453075ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:36.185084Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:34.683069Z","time spent":"1.501998109s","remote":"127.0.0.1:51282","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3152,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/deployments/default/rails\" mod_revision:198464 > success:<request_put:<key:\"/registry/deployments/default/rails\" value_size:3109 >> failure:<request_range:<key:\"/registry/deployments/default/rails\" > >"}
{"level":"info","ts":"2024-07-04T15:04:36.187726Z","caller":"traceutil/trace.go:171","msg":"trace[1944757897] transaction","detail":"{read_only:false; response_revision:201851; number_of_response:1; }","duration":"1.502662408s","start":"2024-07-04T15:04:34.68504Z","end":"2024-07-04T15:04:36.187702Z","steps":["trace[1944757897] 'process raft request'  (duration: 1.499896808s)"],"step_count":1}
{"level":"info","ts":"2024-07-04T15:04:36.188122Z","caller":"traceutil/trace.go:171","msg":"trace[1322277834] linearizableReadLoop","detail":"{readStateIndex:224582; appliedIndex:224578; }","duration":"1.6046045s","start":"2024-07-04T15:04:34.583478Z","end":"2024-07-04T15:04:36.188083Z","steps":["trace[1322277834] 'read index received'  (duration: 198.852206ms)","trace[1322277834] 'applied index is now lower than readState.Index'  (duration: 1.405744475s)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:36.188463Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.697342298s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-07-04T15:04:36.188522Z","caller":"traceutil/trace.go:171","msg":"trace[1503589706] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:201851; }","duration":"1.697466797s","start":"2024-07-04T15:04:34.49104Z","end":"2024-07-04T15:04:36.188507Z","steps":["trace[1503589706] 'agreement among raft nodes before linearized reading'  (duration: 1.697166377s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:36.188566Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:34.490986Z","time spent":"1.697567642s","remote":"127.0.0.1:51032","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":1,"response size":31,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2024-07-04T15:04:36.188586Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:34.685003Z","time spent":"1.502787158s","remote":"127.0.0.1:51094","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":616,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.resolutionrequest.reconciler.00-of-01\" mod_revision:201842 > success:<request_put:<key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.resolutionrequest.reconciler.00-of-01\" value_size:464 >> failure:<request_range:<key:\"/registry/leases/tekton-pipelines/tekton-pipelines-controller.github.com.tektoncd.pipeline.pkg.reconciler.resolutionrequest.reconciler.00-of-01\" > >"}
{"level":"warn","ts":"2024-07-04T15:04:36.582687Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"395.006874ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528478847970091 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/argocd/argocd-application-controller-0\" mod_revision:201664 > success:<request_put:<key:\"/registry/pods/argocd/argocd-application-controller-0\" value_size:12070 >> failure:<request_range:<key:\"/registry/pods/argocd/argocd-application-controller-0\" > >>","response":"size:18"}
{"level":"info","ts":"2024-07-04T15:04:36.585599Z","caller":"traceutil/trace.go:171","msg":"trace[566316287] transaction","detail":"{read_only:false; response_revision:201852; number_of_response:1; }","duration":"1.100031213s","start":"2024-07-04T15:04:35.485488Z","end":"2024-07-04T15:04:36.585519Z","steps":["trace[566316287] 'process raft request'  (duration: 702.103688ms)","trace[566316287] 'compare'  (duration: 101.273447ms)","trace[566316287] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/pods/argocd/argocd-application-controller-0; req_size:12128; } (duration: 201.284936ms)","trace[566316287] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/pods/argocd/argocd-application-controller-0; req_size:12128; } (duration: 92.251053ms)"],"step_count":4}
{"level":"warn","ts":"2024-07-04T15:04:36.585872Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:35.485435Z","time spent":"1.100307687s","remote":"127.0.0.1:50996","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":12131,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/pods/argocd/argocd-application-controller-0\" mod_revision:201664 > success:<request_put:<key:\"/registry/pods/argocd/argocd-application-controller-0\" value_size:12070 >> failure:<request_range:<key:\"/registry/pods/argocd/argocd-application-controller-0\" > >"}
{"level":"info","ts":"2024-07-04T15:04:36.593595Z","caller":"traceutil/trace.go:171","msg":"trace[45824366] transaction","detail":"{read_only:false; response_revision:201853; number_of_response:1; }","duration":"1.098918983s","start":"2024-07-04T15:04:35.494613Z","end":"2024-07-04T15:04:36.593532Z","steps":["trace[45824366] 'process raft request'  (duration: 1.088188254s)"],"step_count":1}
{"level":"info","ts":"2024-07-04T15:04:36.593906Z","caller":"traceutil/trace.go:171","msg":"trace[453117901] transaction","detail":"{read_only:false; response_revision:201854; number_of_response:1; }","duration":"910.734701ms","start":"2024-07-04T15:04:35.68312Z","end":"2024-07-04T15:04:36.593855Z","steps":["trace[453117901] 'process raft request'  (duration: 908.20617ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:04:36.593971Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:35.494539Z","time spent":"1.099259185s","remote":"127.0.0.1:51320","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":5989,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/replicasets/tekton-pipelines/tekton-events-controller-56b58fcf8b\" mod_revision:199059 > success:<request_put:<key:\"/registry/replicasets/tekton-pipelines/tekton-events-controller-56b58fcf8b\" value_size:5907 >> failure:<request_range:<key:\"/registry/replicasets/tekton-pipelines/tekton-events-controller-56b58fcf8b\" > >"}
{"level":"warn","ts":"2024-07-04T15:04:36.594121Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:35.683087Z","time spent":"910.924412ms","remote":"127.0.0.1:50970","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1211,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/tekton-pipelines/tekton-events-controller\" mod_revision:201430 > success:<request_put:<key:\"/registry/services/endpoints/tekton-pipelines/tekton-events-controller\" value_size:1133 >> failure:<request_range:<key:\"/registry/services/endpoints/tekton-pipelines/tekton-events-controller\" > >"}
{"level":"warn","ts":"2024-07-04T15:04:36.783335Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:35.68282Z","time spent":"1.10050983s","remote":"127.0.0.1:51102","response type":"/etcdserverpb.KV/Txn","request count":0,"request size":0,"response count":0,"response size":0,"request content":""}
2024/07/04 15:04:36 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-07-04T15:04:36.793472Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.397499ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528478847970093 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/endpointslices/tekton-pipelines/tekton-events-controller-fplng\" mod_revision:201436 > success:<request_put:<key:\"/registry/endpointslices/tekton-pipelines/tekton-events-controller-fplng\" value_size:1687 >> failure:<request_range:<key:\"/registry/endpointslices/tekton-pipelines/tekton-events-controller-fplng\" > >>","response":"size:18"}
{"level":"info","ts":"2024-07-04T15:04:36.793716Z","caller":"traceutil/trace.go:171","msg":"trace[1296727093] linearizableReadLoop","detail":"{readStateIndex:224588; appliedIndex:224584; }","duration":"605.498108ms","start":"2024-07-04T15:04:36.188182Z","end":"2024-07-04T15:04:36.793681Z","steps":["trace[1296727093] 'read index received'  (duration: 57.772µs)","trace[1296727093] 'applied index is now lower than readState.Index'  (duration: 605.437828ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:36.884001Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"696.265075ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:422"}
{"level":"info","ts":"2024-07-04T15:04:36.88413Z","caller":"traceutil/trace.go:171","msg":"trace[94222447] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:201855; }","duration":"696.455991ms","start":"2024-07-04T15:04:36.187642Z","end":"2024-07-04T15:04:36.884098Z","steps":["trace[94222447] 'agreement among raft nodes before linearized reading'  (duration: 606.732662ms)","trace[94222447] 'range keys from bolt db'  (duration: 89.485987ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:36.884201Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:36.18762Z","time spent":"696.562197ms","remote":"127.0.0.1:50970","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":445,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"warn","ts":"2024-07-04T15:04:37.087243Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"904.699448ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" ","response":"range_response_count:3 size:14419"}
{"level":"info","ts":"2024-07-04T15:04:37.087605Z","caller":"traceutil/trace.go:171","msg":"trace[1826025422] range","detail":"{range_begin:/registry/pods/ingress-nginx/; range_end:/registry/pods/ingress-nginx0; response_count:3; response_revision:201855; }","duration":"905.108545ms","start":"2024-07-04T15:04:36.18245Z","end":"2024-07-04T15:04:37.087558Z","steps":["trace[1826025422] 'agreement among raft nodes before linearized reading'  (duration: 611.307859ms)","trace[1826025422] 'range keys from bolt db'  (duration: 197.750108ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:04:37.088496Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:04:36.18243Z","time spent":"905.24778ms","remote":"127.0.0.1:50996","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":3,"response size":14442,"request content":"key:\"/registry/pods/ingress-nginx/\" range_end:\"/registry/pods/ingress-nginx0\" "}
{"level":"info","ts":"2024-07-04T15:04:37.485303Z","caller":"traceutil/trace.go:171","msg":"trace[161373673] transaction","detail":"{read_only:false; number_of_response:1; response_revision:201856; }","duration":"192.036066ms","start":"2024-07-04T15:04:37.29323Z","end":"2024-07-04T15:04:37.485266Z","steps":["trace[161373673] 'process raft request'  (duration: 191.827333ms)"],"step_count":1}
{"level":"info","ts":"2024-07-04T15:04:37.992044Z","caller":"traceutil/trace.go:171","msg":"trace[1780517258] transaction","detail":"{read_only:false; response_revision:201858; number_of_response:1; }","duration":"100.661643ms","start":"2024-07-04T15:04:37.891341Z","end":"2024-07-04T15:04:37.992003Z","steps":["trace[1780517258] 'process raft request'  (duration: 94.855373ms)"],"step_count":1}
{"level":"info","ts":"2024-07-04T15:04:38.486667Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-07-04T15:04:38.582726Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"warn","ts":"2024-07-04T15:04:38.588238Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-04T15:04:38.588506Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-04T15:04:39.283509Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-07-04T15:04:39.28362Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-07-04T15:04:39.379774Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b2c6679ac05f2cf1","current-leader-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2024-07-04T15:04:39.788633Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-07-04T15:04:39.789397Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2024-07-04T15:04:39.789488Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}


==> etcd [c5a7d23c3462] <==
{"level":"warn","ts":"2024-07-04T15:20:47.169122Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.195927703s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" limit:500 ","response":"range_response_count:1 size:2735"}
{"level":"info","ts":"2024-07-04T15:20:47.169222Z","caller":"traceutil/trace.go:171","msg":"trace[1772549743] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:1; response_revision:202567; }","duration":"1.196074346s","start":"2024-07-04T15:20:45.977803Z","end":"2024-07-04T15:20:47.169195Z","steps":["trace[1772549743] 'agreement among raft nodes before linearized reading'  (duration: 1.104884758s)","trace[1772549743] 'range keys from in-memory index tree'  (duration: 90.311659ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:20:47.169285Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:45.977771Z","time spent":"1.196182163s","remote":"127.0.0.1:36658","response type":"/etcdserverpb.KV/Range","request count":0,"request size":77,"response count":1,"response size":2758,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:47.1699Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.102393563s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-07-04T15:20:47.170281Z","caller":"traceutil/trace.go:171","msg":"trace[1126096978] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:202567; }","duration":"1.10252364s","start":"2024-07-04T15:20:46.07211Z","end":"2024-07-04T15:20:47.169951Z","steps":["trace[1126096978] 'agreement among raft nodes before linearized reading'  (duration: 1.010211545s)","trace[1126096978] 'range keys from in-memory index tree'  (duration: 91.617327ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:20:47.172576Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:46.072952Z","time spent":"1.099944651s","remote":"127.0.0.1:50926","response type":"/etcdserverpb.KV/Range","request count":0,"request size":77,"response count":1,"response size":6531,"request content":"key:\"/registry/argoproj.io/applications/\" range_end:\"/registry/argoproj.io/applications0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:47.173945Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.199083947s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/tekton.dev/stepactions/\" range_end:\"/registry/tekton.dev/stepactions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-07-04T15:20:47.174077Z","caller":"traceutil/trace.go:171","msg":"trace[2112075157] range","detail":"{range_begin:/registry/tekton.dev/stepactions/; range_end:/registry/tekton.dev/stepactions0; response_count:0; response_revision:202567; }","duration":"1.199271498s","start":"2024-07-04T15:20:45.979452Z","end":"2024-07-04T15:20:47.174041Z","steps":["trace[2112075157] 'agreement among raft nodes before linearized reading'  (duration: 1.103195076s)","trace[2112075157] 'count revisions from in-memory index tree'  (duration: 95.802433ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:20:47.174166Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:45.979432Z","time spent":"1.199389797s","remote":"127.0.0.1:50942","response type":"/etcdserverpb.KV/Range","request count":0,"request size":72,"response count":0,"response size":29,"request content":"key:\"/registry/tekton.dev/stepactions/\" range_end:\"/registry/tekton.dev/stepactions0\" count_only:true "}
{"level":"warn","ts":"2024-07-04T15:20:47.175348Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:46.07204Z","time spent":"1.103015535s","remote":"127.0.0.1:57056","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-07-04T15:20:47.17598Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.006455348s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/pvc-116e1b74-f983-4802-a104-95c40a4e413c\" ","response":"range_response_count:1 size:1268"}
{"level":"warn","ts":"2024-07-04T15:20:47.177005Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.117725ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528479151966439 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/default/kubernetes\" mod_revision:201969 > success:<request_put:<key:\"/registry/services/endpoints/default/kubernetes\" value_size:298 >> failure:<request_range:<key:\"/registry/services/endpoints/default/kubernetes\" > >>","response":"size:18"}
{"level":"info","ts":"2024-07-04T15:20:47.177128Z","caller":"traceutil/trace.go:171","msg":"trace[1491122959] linearizableReadLoop","detail":"{readStateIndex:225387; appliedIndex:225386; }","duration":"100.078876ms","start":"2024-07-04T15:20:47.077029Z","end":"2024-07-04T15:20:47.177108Z","steps":["trace[1491122959] 'read index received'  (duration: 989.541µs)","trace[1491122959] 'applied index is now lower than readState.Index'  (duration: 99.087937ms)"],"step_count":2}
{"level":"info","ts":"2024-07-04T15:20:47.267072Z","caller":"traceutil/trace.go:171","msg":"trace[1584893987] transaction","detail":"{read_only:false; response_revision:202568; number_of_response:1; }","duration":"297.383985ms","start":"2024-07-04T15:20:46.969628Z","end":"2024-07-04T15:20:47.267012Z","steps":["trace[1584893987] 'process raft request'  (duration: 107.088459ms)","trace[1584893987] 'compare'  (duration: 93.547796ms)"],"step_count":2}
{"level":"info","ts":"2024-07-04T15:20:47.267378Z","caller":"traceutil/trace.go:171","msg":"trace[557938959] range","detail":"{range_begin:/registry/persistentvolumes/pvc-116e1b74-f983-4802-a104-95c40a4e413c; range_end:; response_count:1; response_revision:202567; }","duration":"1.006641867s","start":"2024-07-04T15:20:46.174103Z","end":"2024-07-04T15:20:47.176062Z","steps":["trace[557938959] 'agreement among raft nodes before linearized reading'  (duration: 908.187099ms)","trace[557938959] 'range keys from in-memory index tree'  (duration: 97.585427ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:20:47.267571Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:46.174062Z","time spent":"1.0981254s","remote":"127.0.0.1:36606","response type":"/etcdserverpb.KV/Range","request count":0,"request size":70,"response count":1,"response size":1291,"request content":"key:\"/registry/persistentvolumes/pvc-116e1b74-f983-4802-a104-95c40a4e413c\" "}
{"level":"warn","ts":"2024-07-04T15:20:47.271974Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"400.741547ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/tekton.dev/pipelines/\" range_end:\"/registry/tekton.dev/pipelines0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-07-04T15:20:47.272955Z","caller":"traceutil/trace.go:171","msg":"trace[1569094040] range","detail":"{range_begin:/registry/tekton.dev/pipelines/; range_end:/registry/tekton.dev/pipelines0; response_count:0; response_revision:202568; }","duration":"400.920829ms","start":"2024-07-04T15:20:46.871125Z","end":"2024-07-04T15:20:47.272063Z","steps":["trace[1569094040] 'agreement among raft nodes before linearized reading'  (duration: 306.006766ms)","trace[1569094040] 'count revisions from in-memory index tree'  (duration: 94.670486ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:20:47.273121Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:46.870717Z","time spent":"402.346816ms","remote":"127.0.0.1:50972","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":6,"response size":31,"request content":"key:\"/registry/tekton.dev/pipelines/\" range_end:\"/registry/tekton.dev/pipelines0\" count_only:true "}
{"level":"warn","ts":"2024-07-04T15:20:47.27382Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.103469291s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" limit:500 ","response":"range_response_count:13 size:14398"}
{"level":"info","ts":"2024-07-04T15:20:47.273924Z","caller":"traceutil/trace.go:171","msg":"trace[1729329889] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:13; response_revision:202567; }","duration":"1.103630623s","start":"2024-07-04T15:20:46.174944Z","end":"2024-07-04T15:20:47.273892Z","steps":["trace[1729329889] 'agreement among raft nodes before linearized reading'  (duration: 907.335311ms)","trace[1729329889] 'range keys from in-memory index tree'  (duration: 94.222223ms)","trace[1729329889] 'range keys from bolt db'  (duration: 101.920581ms)"],"step_count":3}
{"level":"warn","ts":"2024-07-04T15:20:47.274054Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:46.174914Z","time spent":"1.103739357s","remote":"127.0.0.1:36874","response type":"/etcdserverpb.KV/Range","request count":0,"request size":51,"response count":13,"response size":14421,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:47.370946Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.401696421s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" limit:500 ","response":"range_response_count:23 size:130442"}
{"level":"info","ts":"2024-07-04T15:20:47.371045Z","caller":"traceutil/trace.go:171","msg":"trace[1011534631] range","detail":"{range_begin:/registry/replicasets/; range_end:/registry/replicasets0; response_count:23; response_revision:202565; }","duration":"1.401862799s","start":"2024-07-04T15:20:45.973845Z","end":"2024-07-04T15:20:47.371026Z","steps":["trace[1011534631] 'range keys from bolt db'  (duration: 1.398139338s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:47.371096Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:45.973828Z","time spent":"1.401933573s","remote":"127.0.0.1:36908","response type":"/etcdserverpb.KV/Range","request count":0,"request size":51,"response count":23,"response size":130465,"request content":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:47.469233Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.488495632s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/tekton.dev/tasks/\" range_end:\"/registry/tekton.dev/tasks0\" limit:500 ","response":"range_response_count:10 size:26798"}
{"level":"info","ts":"2024-07-04T15:20:47.469677Z","caller":"traceutil/trace.go:171","msg":"trace[894347971] range","detail":"{range_begin:/registry/tekton.dev/tasks/; range_end:/registry/tekton.dev/tasks0; response_count:10; response_revision:202567; }","duration":"1.488853073s","start":"2024-07-04T15:20:45.98536Z","end":"2024-07-04T15:20:47.46953Z","steps":["trace[894347971] 'agreement among raft nodes before linearized reading'  (duration: 1.096966742s)","trace[894347971] 'range keys from in-memory index tree'  (duration: 90.375213ms)","trace[894347971] 'range keys from bolt db'  (duration: 301.155606ms)"],"step_count":3}
{"level":"warn","ts":"2024-07-04T15:20:47.470036Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:45.985339Z","time spent":"1.489114007s","remote":"127.0.0.1:50996","response type":"/etcdserverpb.KV/Range","request count":0,"request size":61,"response count":10,"response size":26821,"request content":"key:\"/registry/tekton.dev/tasks/\" range_end:\"/registry/tekton.dev/tasks0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:47.472194Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.49221896s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" limit:500 ","response":"range_response_count:8 size:5420"}
{"level":"info","ts":"2024-07-04T15:20:47.472308Z","caller":"traceutil/trace.go:171","msg":"trace[342510349] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:8; response_revision:202567; }","duration":"1.492369055s","start":"2024-07-04T15:20:45.984593Z","end":"2024-07-04T15:20:47.47228Z","steps":["trace[342510349] 'agreement among raft nodes before linearized reading'  (duration: 1.097742597s)","trace[342510349] 'range keys from in-memory index tree'  (duration: 95.705561ms)","trace[342510349] 'range keys from bolt db'  (duration: 298.442893ms)"],"step_count":3}
{"level":"warn","ts":"2024-07-04T15:20:47.472365Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:45.984539Z","time spent":"1.492486771s","remote":"127.0.0.1:36876","response type":"/etcdserverpb.KV/Range","request count":0,"request size":83,"response count":8,"response size":5443,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:47.569065Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"297.698333ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238528479151966442 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/prometheus/prometheus-kube-state-metrics-67848d7455-d7dpt.17df0b3578ba93cb\" mod_revision:0 > success:<request_put:<key:\"/registry/events/prometheus/prometheus-kube-state-metrics-67848d7455-d7dpt.17df0b3578ba93cb\" value_size:801 lease:3238528479151966399 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2024-07-04T15:20:47.569206Z","caller":"traceutil/trace.go:171","msg":"trace[268748507] linearizableReadLoop","detail":"{readStateIndex:225388; appliedIndex:225387; }","duration":"387.035886ms","start":"2024-07-04T15:20:47.182108Z","end":"2024-07-04T15:20:47.569144Z","steps":["trace[268748507] 'read index received'  (duration: 87.88146ms)","trace[268748507] 'applied index is now lower than readState.Index'  (duration: 299.151248ms)"],"step_count":2}
{"level":"info","ts":"2024-07-04T15:20:47.569331Z","caller":"traceutil/trace.go:171","msg":"trace[738744500] transaction","detail":"{read_only:false; response_revision:202569; number_of_response:1; }","duration":"389.878019ms","start":"2024-07-04T15:20:47.179434Z","end":"2024-07-04T15:20:47.569312Z","steps":["trace[738744500] 'process raft request'  (duration: 91.858174ms)","trace[738744500] 'compare'  (duration: 297.151725ms)"],"step_count":2}
{"level":"warn","ts":"2024-07-04T15:20:47.569435Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:47.179394Z","time spent":"389.983896ms","remote":"127.0.0.1:36540","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":910,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/events/prometheus/prometheus-kube-state-metrics-67848d7455-d7dpt.17df0b3578ba93cb\" mod_revision:0 > success:<request_put:<key:\"/registry/events/prometheus/prometheus-kube-state-metrics-67848d7455-d7dpt.17df0b3578ba93cb\" value_size:801 lease:3238528479151966399 >> failure:<>"}
{"level":"warn","ts":"2024-07-04T15:20:47.569928Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"300.304565ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-07-04T15:20:47.570386Z","caller":"traceutil/trace.go:171","msg":"trace[1829598023] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:202569; }","duration":"300.455508ms","start":"2024-07-04T15:20:47.269542Z","end":"2024-07-04T15:20:47.569997Z","steps":["trace[1829598023] 'agreement among raft nodes before linearized reading'  (duration: 299.947525ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:47.570599Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.706817ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/argocd/app-storage\" ","response":"range_response_count:1 size:1765"}
{"level":"warn","ts":"2024-07-04T15:20:47.57062Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:47.269498Z","time spent":"301.085215ms","remote":"127.0.0.1:36860","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":31,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"info","ts":"2024-07-04T15:20:47.570655Z","caller":"traceutil/trace.go:171","msg":"trace[26270505] range","detail":"{range_begin:/registry/persistentvolumeclaims/argocd/app-storage; range_end:; response_count:1; response_revision:202569; }","duration":"101.816303ms","start":"2024-07-04T15:20:47.468805Z","end":"2024-07-04T15:20:47.570622Z","steps":["trace[26270505] 'agreement among raft nodes before linearized reading'  (duration: 101.547348ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:47.570902Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"388.817342ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-07-04T15:20:47.570931Z","caller":"traceutil/trace.go:171","msg":"trace[1711051873] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:202569; }","duration":"388.875546ms","start":"2024-07-04T15:20:47.182048Z","end":"2024-07-04T15:20:47.570923Z","steps":["trace[1711051873] 'agreement among raft nodes before linearized reading'  (duration: 388.787805ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:47.571002Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:47.182023Z","time spent":"388.967955ms","remote":"127.0.0.1:57072","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-07-04T15:20:47.571085Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"197.984608ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:481"}
{"level":"info","ts":"2024-07-04T15:20:47.571115Z","caller":"traceutil/trace.go:171","msg":"trace[1972265553] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:202569; }","duration":"198.053049ms","start":"2024-07-04T15:20:47.373052Z","end":"2024-07-04T15:20:47.571105Z","steps":["trace[1972265553] 'agreement among raft nodes before linearized reading'  (duration: 197.94729ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:47.571544Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"203.268622ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/kube-apiserver-minikube\" ","response":"range_response_count:1 size:7552"}
{"level":"info","ts":"2024-07-04T15:20:47.571653Z","caller":"traceutil/trace.go:171","msg":"trace[324196768] range","detail":"{range_begin:/registry/pods/kube-system/kube-apiserver-minikube; range_end:; response_count:1; response_revision:202569; }","duration":"203.690194ms","start":"2024-07-04T15:20:47.36794Z","end":"2024-07-04T15:20:47.57163Z","steps":["trace[324196768] 'agreement among raft nodes before linearized reading'  (duration: 203.253938ms)"],"step_count":1}
{"level":"info","ts":"2024-07-04T15:20:48.270668Z","caller":"traceutil/trace.go:171","msg":"trace[191255934] transaction","detail":"{read_only:false; response_revision:202570; number_of_response:1; }","duration":"493.271216ms","start":"2024-07-04T15:20:47.777318Z","end":"2024-07-04T15:20:48.270589Z","steps":["trace[191255934] 'process raft request'  (duration: 489.762158ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:48.271432Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:47.777284Z","time spent":"493.752178ms","remote":"127.0.0.1:36720","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":427,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/endpointslices/default/kubernetes\" mod_revision:201975 > success:<request_put:<key:\"/registry/endpointslices/default/kubernetes\" value_size:376 >> failure:<request_range:<key:\"/registry/endpointslices/default/kubernetes\" > >"}
{"level":"warn","ts":"2024-07-04T15:20:48.373998Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.05195ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/tekton.dev/clustertasks/\" range_end:\"/registry/tekton.dev/clustertasks0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-07-04T15:20:48.374107Z","caller":"traceutil/trace.go:171","msg":"trace[1093967411] range","detail":"{range_begin:/registry/tekton.dev/clustertasks/; range_end:/registry/tekton.dev/clustertasks0; response_count:0; response_revision:202570; }","duration":"100.221878ms","start":"2024-07-04T15:20:48.273859Z","end":"2024-07-04T15:20:48.374081Z","steps":["trace[1093967411] 'count revisions from in-memory index tree'  (duration: 99.9304ms)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:48.477471Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.406314141s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" limit:500 ","response":"range_response_count:9 size:71779"}
{"level":"info","ts":"2024-07-04T15:20:48.477598Z","caller":"traceutil/trace.go:171","msg":"trace[420798957] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:9; response_revision:202567; }","duration":"2.40654777s","start":"2024-07-04T15:20:46.07568Z","end":"2024-07-04T15:20:48.477545Z","steps":["trace[420798957] 'agreement among raft nodes before linearized reading'  (duration: 1.006630208s)","trace[420798957] 'range keys from in-memory index tree'  (duration: 90.025778ms)","trace[420798957] 'range keys from bolt db'  (duration: 1.309642827s)"],"step_count":3}
{"level":"warn","ts":"2024-07-04T15:20:48.477653Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:46.075567Z","time spent":"2.406750749s","remote":"127.0.0.1:36554","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":9,"response size":71802,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:49.473181Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.198968582s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" limit:500 ","response":"range_response_count:20 size:165485"}
{"level":"info","ts":"2024-07-04T15:20:49.47328Z","caller":"traceutil/trace.go:171","msg":"trace[1907059317] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:20; response_revision:202570; }","duration":"1.199120027s","start":"2024-07-04T15:20:48.27414Z","end":"2024-07-04T15:20:49.47326Z","steps":["trace[1907059317] 'range keys from bolt db'  (duration: 1.19266008s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:49.473665Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:48.274124Z","time spent":"1.199189073s","remote":"127.0.0.1:36886","response type":"/etcdserverpb.KV/Range","request count":0,"request size":51,"response count":20,"response size":165508,"request content":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" limit:500 "}
{"level":"warn","ts":"2024-07-04T15:20:52.872693Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"4.599570738s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" limit:500 ","response":"range_response_count:13 size:838661"}
{"level":"info","ts":"2024-07-04T15:20:52.872812Z","caller":"traceutil/trace.go:171","msg":"trace[362532591] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:13; response_revision:202570; }","duration":"4.59983691s","start":"2024-07-04T15:20:48.272953Z","end":"2024-07-04T15:20:52.87279Z","steps":["trace[362532591] 'range keys from bolt db'  (duration: 4.593850186s)"],"step_count":1}
{"level":"warn","ts":"2024-07-04T15:20:52.872931Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-07-04T15:20:48.27287Z","time spent":"4.599981668s","remote":"127.0.0.1:36520","response type":"/etcdserverpb.KV/Range","request count":0,"request size":121,"response count":13,"response size":838684,"request content":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" limit:500 "}


==> kernel <==
 15:21:09 up 37 min,  0 users,  load average: 16.56, 16.72, 14.39
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [61179c7bfbc7] <==
W0704 15:04:39.887692       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.887693       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.887700       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.888299       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.888299       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.888303       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.888534       1 logging.go:59] [core] [Channel #238 SubChannel #239] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.979580       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.979884       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.979829       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.980136       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.980523       1 logging.go:59] [core] [Channel #202 SubChannel #203] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.980544       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.980564       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.980634       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.980380       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.982567       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.982568       1 logging.go:59] [core] [Channel #232 SubChannel #233] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.982888       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983381       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983566       1 logging.go:59] [core] [Channel #241 SubChannel #242] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983597       1 logging.go:59] [core] [Channel #229 SubChannel #230] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983615       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983666       1 logging.go:59] [core] [Channel #58 SubChannel #59] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983837       1 logging.go:59] [core] [Channel #211 SubChannel #212] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983886       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983996       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:39.983967       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.079635       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.079755       1 logging.go:59] [core] [Channel #235 SubChannel #236] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.079794       1 logging.go:59] [core] [Channel #223 SubChannel #224] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.079823       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.080111       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.080308       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.080341       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.081108       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.082117       1 logging.go:59] [core] [Channel #214 SubChannel #215] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.082118       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.082373       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.080718       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.082684       1 logging.go:59] [core] [Channel #226 SubChannel #227] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.082683       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083374       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083420       1 logging.go:59] [core] [Channel #217 SubChannel #218] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083620       1 logging.go:59] [core] [Channel #196 SubChannel #197] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083677       1 logging.go:59] [core] [Channel #205 SubChannel #206] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083710       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083868       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083946       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084089       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084102       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.083774       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084229       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084182       1 logging.go:59] [core] [Channel #208 SubChannel #209] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084346       1 logging.go:59] [core] [Channel #199 SubChannel #200] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084566       1 logging.go:59] [core] [Channel #220 SubChannel #221] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084672       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084229       1 logging.go:59] [core] [Channel #190 SubChannel #191] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.084983       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0704 15:04:40.090654       1 controller.go:136] slow openapi aggregation of "appprojects.argoproj.io": 2.102890832s


==> kube-apiserver [deaa802192db] <==
Trace[2063161878]: ---"Writing http response done" count:10 102ms (15:20:48.070)
Trace[2063161878]: [2.193939771s] [2.193939771s] END
I0704 15:20:48.275072       1 trace.go:236] Trace[120768391]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:db4fa38d-1137-438c-82e9-f6d1c54d9197,client:::1,api-group:discovery.k8s.io,api-version:v1,name:kubernetes,subresource:,namespace:default,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (04-Jul-2024 15:20:47.668) (total time: 605ms):
Trace[120768391]: ["GuaranteedUpdate etcd3" audit-id:db4fa38d-1137-438c-82e9-f6d1c54d9197,key:/endpointslices/default/kubernetes,type:*discovery.EndpointSlice,resource:endpointslices.discovery.k8s.io 605ms (15:20:47.669)
Trace[120768391]:  ---"About to Encode" 105ms (15:20:47.775)
Trace[120768391]:  ---"Txn call completed" 498ms (15:20:48.274)]
Trace[120768391]: [605.963916ms] [605.963916ms] END
I0704 15:20:48.279217       1 dynamic_serving_content.go:146] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0704 15:20:48.370164       1 controller.go:117] Shutting down OpenAPI V3 controller
I0704 15:20:48.370375       1 customresource_discovery_controller.go:325] Shutting down DiscoveryController
I0704 15:20:48.373782       1 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I0704 15:20:48.378345       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0704 15:20:48.467471       1 autoregister_controller.go:165] Shutting down autoregister controller
I0704 15:20:48.467916       1 controller.go:167] Shutting down OpenAPI controller
I0704 15:20:48.566890       1 storage_flowcontrol.go:187] APF bootstrap ensurer is exiting
I0704 15:20:48.567038       1 cluster_authentication_trust_controller.go:463] Shutting down cluster_authentication_trust_controller controller
I0704 15:20:48.568919       1 naming_controller.go:302] Shutting down NamingConditionController
I0704 15:20:48.569148       1 nonstructuralschema_controller.go:204] Shutting down NonStructuralSchemaConditionController
I0704 15:20:48.569395       1 crdregistration_controller.go:142] Shutting down crd-autoregister controller
I0704 15:20:48.573375       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0704 15:20:48.574897       1 apiapproval_controller.go:198] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I0704 15:20:48.575141       1 crd_finalizer.go:278] Shutting down CRDFinalizer
I0704 15:20:48.575402       1 apiservice_controller.go:131] Shutting down APIServiceRegistrationController
I0704 15:20:48.575811       1 available_controller.go:439] Shutting down AvailableConditionController
I0704 15:20:48.575953       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I0704 15:20:48.576082       1 system_namespaces_controller.go:77] Shutting down system namespaces controller
I0704 15:20:48.576206       1 apf_controller.go:386] Shutting down API Priority and Fairness config worker
I0704 15:20:48.576668       1 establishing_controller.go:87] Shutting down EstablishingController
I0704 15:20:48.577326       1 dynamic_serving_content.go:146] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0704 15:20:48.577369       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0704 15:20:48.577440       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0704 15:20:48.577828       1 controller.go:84] Shutting down OpenAPI AggregationController
I0704 15:20:48.583242       1 secure_serving.go:258] Stopped listening on [::]:8443
I0704 15:20:48.583360       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0704 15:20:48.672475       1 controller.go:129] Ending legacy_token_tracking_controller
I0704 15:20:48.672536       1 controller.go:130] Shutting down legacy_token_tracking_controller
I0704 15:20:48.869106       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I0704 15:20:49.472128       1 trace.go:236] Trace[517666968]: "List" accept:application/json,audit-id:b4fd9374-6cf6-46c1-aa4a-ba0e552b9302,client:10.244.1.111,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:secrets,scope:cluster,url:/api/v1/secrets,user-agent:argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (04-Jul-2024 15:20:45.985) (total time: 3491ms):
Trace[517666968]: ["List(recursive=true) etcd3" audit-id:b4fd9374-6cf6-46c1-aa4a-ba0e552b9302,key:/secrets,resourceVersion:,resourceVersionMatch:,limit:500,continue: 3490ms (15:20:45.985)]
Trace[517666968]: ---"Writing http response done" count:9 103ms (15:20:49.471)
Trace[517666968]: [3.491020886s] [3.491020886s] END
I0704 15:20:49.579601       1 trace.go:236] Trace[240433996]: "List" accept:application/json,audit-id:aec89905-f25e-4d31-8818-42c00009f29d,client:10.244.1.111,api-group:apps,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:replicasets,scope:cluster,url:/apis/apps/v1/replicasets,user-agent:argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (04-Jul-2024 15:20:45.685) (total time: 3898ms):
Trace[240433996]: ["List(recursive=true) etcd3" audit-id:aec89905-f25e-4d31-8818-42c00009f29d,key:/replicasets,resourceVersion:,resourceVersionMatch:,limit:500,continue: 3812ms (15:20:45.771)]
Trace[240433996]: ---"Writing http response done" count:23 1012ms (15:20:49.579)
Trace[240433996]: [3.898888809s] [3.898888809s] END
I0704 15:20:52.382612       1 trace.go:236] Trace[31186809]: "List" accept:application/json,audit-id:df6627bc-115a-41de-8e3e-b1ebb45f16ab,client:10.244.1.111,api-group:apps,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:deployments,scope:cluster,url:/apis/apps/v1/deployments,user-agent:argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (04-Jul-2024 15:20:47.981) (total time: 4400ms):
Trace[31186809]: ["List(recursive=true) etcd3" audit-id:df6627bc-115a-41de-8e3e-b1ebb45f16ab,key:/deployments,resourceVersion:,resourceVersionMatch:,limit:500,continue: 4400ms (15:20:47.981)]
Trace[31186809]: ---"Writing http response done" count:20 1510ms (15:20:52.382)
Trace[31186809]: [4.400808725s] [4.400808725s] END
W0704 15:20:53.978424       1 controller.go:136] slow openapi aggregation of "applicationsets.argoproj.io": 23.411072603s
I0704 15:20:58.947469       1 trace.go:236] Trace[217004872]: "List" accept:application/json,audit-id:d0f1eac7-dd48-4ca0-9668-86d2aae7421d,client:10.244.1.111,api-group:apiextensions.k8s.io,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:customresourcedefinitions,scope:cluster,url:/apis/apiextensions.k8s.io/v1/customresourcedefinitions,user-agent:argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (04-Jul-2024 15:20:48.069) (total time: 10878ms):
Trace[217004872]: ["List(recursive=true) etcd3" audit-id:d0f1eac7-dd48-4ca0-9668-86d2aae7421d,key:/apiextensions.k8s.io/customresourcedefinitions,resourceVersion:,resourceVersionMatch:,limit:500,continue: 10878ms (15:20:48.069)]
Trace[217004872]: ---"Writing http response done" count:13 2375ms (15:20:58.947)
Trace[217004872]: [10.878287019s] [10.878287019s] END
I0704 15:20:58.947923       1 controller.go:157] Shutting down quota evaluator
I0704 15:20:58.948366       1 controller.go:176] quota evaluator worker shutdown
I0704 15:20:58.948374       1 controller.go:176] quota evaluator worker shutdown
I0704 15:20:58.948378       1 controller.go:176] quota evaluator worker shutdown
I0704 15:20:58.948370       1 controller.go:176] quota evaluator worker shutdown
I0704 15:20:58.948373       1 controller.go:176] quota evaluator worker shutdown


==> kube-controller-manager [3bf95a2bd662] <==
command /bin/bash -c "docker logs --tail 60 3bf95a2bd662" failed with error: /bin/bash -c "docker logs --tail 60 3bf95a2bd662": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: 3bf95a2bd662


==> kube-controller-manager [547ca744ed81] <==
E0704 15:13:43.536838       1 node_lifecycle_controller.go:973] "Error updating node" err="the server was unable to return a response in the time allotted, but may still be processing the request (put nodes minikube)" logger="node-lifecycle-controller" node="minikube"
I0704 15:13:57.733003       1 trace.go:236] Trace[1338915498]: "DeltaFIFO Pop Process" ID:tekton-pipelines/tekton-pipelines-webhook,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 15:13:56.032) (total time: 102ms):
Trace[1338915498]: [102.982373ms] [102.982373ms] END
I0704 15:13:59.934793       1 trace.go:236] Trace[374610210]: "DeltaFIFO Pop Process" ID:default/postgres,Depth:27,Reason:slow event handlers blocking the queue (04-Jul-2024 15:13:58.741) (total time: 491ms):
Trace[374610210]: [491.903247ms] [491.903247ms] END
I0704 15:14:00.138887       1 trace.go:236] Trace[50595550]: "DeltaFIFO Pop Process" ID:default/rails,Depth:26,Reason:slow event handlers blocking the queue (04-Jul-2024 15:14:00.033) (total time: 104ms):
Trace[50595550]: [104.136391ms] [104.136391ms] END
I0704 15:14:00.737595       1 trace.go:236] Trace[55972082]: "DeltaFIFO Pop Process" ID:argocd/argocd-server,Depth:20,Reason:slow event handlers blocking the queue (04-Jul-2024 15:14:00.139) (total time: 598ms):
Trace[55972082]: [598.51174ms] [598.51174ms] END
I0704 15:14:27.232133       1 trace.go:236] Trace[377708523]: "DeltaFIFO Pop Process" ID:prometheus/prometheus-alertmanager,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 15:14:26.534) (total time: 196ms):
Trace[377708523]: [196.938816ms] [196.938816ms] END
I0704 15:14:57.935343       1 trace.go:236] Trace[507575686]: "DeltaFIFO Pop Process" ID:argocd/argocd-notifications-controller-metrics,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 15:14:57.037) (total time: 194ms):
Trace[507575686]: [194.153861ms] [194.153861ms] END
I0704 15:14:58.932460       1 trace.go:236] Trace[1739314931]: "DeltaFIFO Pop Process" ID:default/kubernetes,Depth:27,Reason:slow event handlers blocking the queue (04-Jul-2024 15:14:57.943) (total time: 195ms):
Trace[1739314931]: [195.195911ms] [195.195911ms] END
I0704 15:14:59.641040       1 trace.go:236] Trace[1870033020]: "DeltaFIFO Pop Process" ID:prometheus/prometheus-alertmanager,Depth:26,Reason:slow event handlers blocking the queue (04-Jul-2024 15:14:59.135) (total time: 205ms):
Trace[1870033020]: [205.826867ms] [205.826867ms] END
E0704 15:15:24.940392       1 horizontal.go:783] failed to update status for tekton-pipelines-webhook: Timeout: request did not complete within requested timeout - context deadline exceeded
E0704 15:15:23.840423       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" logger="horizontal-pod-autoscaler-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook.17df0b5b9a20a4f3  tekton-pipelines    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:HorizontalPodAutoscaler,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook,UID:a8bec02f-410c-4412-be4d-8571f8b3a5e0,APIVersion:autoscaling/v2,ResourceVersion:198820,FieldPath:,},Reason:FailedGetScale,Message:Get \"https://192.168.58.2:8443/apis/apps/v1/namespaces/tekton-pipelines/deployments/tekton-pipelines-webhook/scale\": stream error: stream ID 389; INTERNAL_ERROR; received from peer,Source:EventSource{Component:horizontal-pod-autoscaler,Host:,},FirstTimestamp:2024-07-04 15:14:30.734578931 +0000 UTC m=+284.603014752,LastTimestamp:2024-07-04 15:14:30.734578931 +0000 UTC m=+284.603014752,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:horizontal-pod-autoscaler,ReportingInstance:,}"
E0704 15:15:27.236837       1 horizontal.go:270] failed to query scale subresource for Deployment/tekton-pipelines/tekton-pipelines-webhook: Get "https://192.168.58.2:8443/apis/apps/v1/namespaces/tekton-pipelines/deployments/tekton-pipelines-webhook/scale": stream error: stream ID 389; INTERNAL_ERROR; received from peer
E0704 15:15:32.731282       1 node_lifecycle_controller.go:973] "Error updating node" err="Timeout: request did not complete within requested timeout - context deadline exceeded" logger="node-lifecycle-controller" node="minikube"
I0704 15:15:34.532585       1 request.go:697] Waited for 1.201219347s due to client-side throttling, not priority and fairness, request: GET:https://192.168.58.2:8443/api/v1/persistentvolumes/pvc-2069aaf8-ab77-4a73-9500-2f20275e1d51
E0704 15:16:06.834236       1 horizontal.go:270] failed to compute desired number of replicas based on listed metrics for Deployment/tekton-pipelines/tekton-pipelines-webhook: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I0704 15:16:29.340923       1 trace.go:236] Trace[2050503701]: "DeltaFIFO Pop Process" ID:argocd/argocd-applicationset-controller,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 15:16:28.432) (total time: 103ms):
Trace[2050503701]: [103.777524ms] [103.777524ms] END
I0704 15:17:01.215021       1 trace.go:236] Trace[1912817644]: "DeltaFIFO Pop Process" ID:prometheus/prometheus-alertmanager,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 15:16:59.118) (total time: 204ms):
Trace[1912817644]: [204.215872ms] [204.215872ms] END
I0704 15:17:02.725363       1 trace.go:236] Trace[1233701535]: "DeltaFIFO Pop Process" ID:kube-system/kube-dns,Depth:27,Reason:slow event handlers blocking the queue (04-Jul-2024 15:17:01.622) (total time: 495ms):
Trace[1233701535]: [495.377973ms] [495.377973ms] END
I0704 15:17:07.015109       1 request.go:697] Waited for 1.205277715s due to client-side throttling, not priority and fairness, request: GET:https://192.168.58.2:8443/api/v1/persistentvolumes/pvc-2069aaf8-ab77-4a73-9500-2f20275e1d51
E0704 15:17:18.717886       1 horizontal.go:270] failed to compute desired number of replicas based on listed metrics for Deployment/tekton-pipelines/tekton-pipelines-webhook: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I0704 15:17:27.714927       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="argocd/argocd-application-controller-0" err="Timeout: request did not complete within requested timeout - context deadline exceeded"
I0704 15:17:28.617322       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"tekton-pipelines-resolvers", Name:"tekton-pipelines-remote-resolvers", UID:"af68621d-e8ca-48f3-b7fe-7e8bc24f4ea3", APIVersion:"v1", ResourceVersion:"33481", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers: failed to update tekton-pipelines-remote-resolvers-2b8hf EndpointSlice for Service tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers: Timeout: request did not complete within requested timeout - context deadline exceeded
E0704 15:17:27.723060       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{tekton-pipelines-remote-resolvers-659495bc6-9kpvm.17df0b7949253bd4  tekton-pipelines-resolvers    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:tekton-pipelines-resolvers,Name:tekton-pipelines-remote-resolvers-659495bc6-9kpvm,UID:2fdeb80a-d8fb-4f38-af0a-501cfc155fa9,APIVersion:v1,ResourceVersion:202097,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2024-07-04 15:16:38.224944084 +0000 UTC m=+412.100335605,LastTimestamp:2024-07-04 15:16:38.224944084 +0000 UTC m=+412.100335605,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
I0704 15:17:29.713603       1 endpointslice_controller.go:311] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers" err="failed to update tekton-pipelines-remote-resolvers-2b8hf EndpointSlice for Service tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers: Timeout: request did not complete within requested timeout - context deadline exceeded"
E0704 15:18:18.103722       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpointslice-controller" event="&Event{ObjectMeta:{tekton-pipelines-remote-resolvers.17df0b84e1430844  tekton-pipelines-resolvers    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:tekton-pipelines-resolvers,Name:tekton-pipelines-remote-resolvers,UID:af68621d-e8ca-48f3-b7fe-7e8bc24f4ea3,APIVersion:v1,ResourceVersion:33481,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers: failed to update tekton-pipelines-remote-resolvers-2b8hf EndpointSlice for Service tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers: Timeout: request did not complete within requested timeout - context deadline exceeded,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2024-07-04 15:17:28.021674052 +0000 UTC m=+461.912570261,LastTimestamp:2024-07-04 15:17:28.021674052 +0000 UTC m=+461.912570261,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
E0704 15:18:18.103836       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{argocd-application-controller-0.17df0b84d4e5c05d  argocd    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:argocd,Name:argocd-application-controller-0,UID:14147cc9-7ba7-4e83-8a50-7bb2e8fad632,APIVersion:v1,ResourceVersion:202299,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2024-07-04 15:17:27.814234205 +0000 UTC m=+461.705130408,LastTimestamp:2024-07-04 15:17:27.814234205 +0000 UTC m=+461.705130408,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
E0704 15:18:18.095806       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpoints-controller" event="&Event{ObjectMeta:{tekton-pipelines-remote-resolvers.17df0b83a4a62c3b  tekton-pipelines-resolvers    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:tekton-pipelines-resolvers,Name:tekton-pipelines-remote-resolvers,UID:bb8c5114-167f-4fa1-ac44-f5cd5304e7b1,APIVersion:v1,ResourceVersion:202214,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers: Timeout: request did not complete within requested timeout - context deadline exceeded,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2024-07-04 15:17:22.709793851 +0000 UTC m=+456.600690140,LastTimestamp:2024-07-04 15:17:22.709793851 +0000 UTC m=+456.600690140,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
E0704 15:18:18.096539       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="horizontal-pod-autoscaler-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook.17df0b25505f58d6  tekton-pipelines   202486 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:HorizontalPodAutoscaler,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook,UID:a8bec02f-410c-4412-be4d-8571f8b3a5e0,APIVersion:autoscaling/v2,ResourceVersion:198820,FieldPath:,},Reason:FailedGetResourceMetric,Message:failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io),Source:EventSource{Component:horizontal-pod-autoscaler,Host:,},FirstTimestamp:2024-07-04 15:10:37 +0000 UTC,LastTimestamp:2024-07-04 15:17:17.415026233 +0000 UTC m=+451.305922439,Count:6,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:horizontal-pod-autoscaler,ReportingInstance:,}"
I0704 15:18:22.998302       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="argocd/argocd-repo-server-f965fdfcf-fjjnx" err="Timeout: request did not complete within requested timeout - context deadline exceeded"
I0704 15:18:35.100893       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers-659495bc6" duration="1m59.902164334s"
I0704 15:18:37.102416       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers-659495bc6" duration="1.306100611s"
I0704 15:19:01.095317       1 trace.go:236] Trace[1015974457]: "DeltaFIFO Pop Process" ID:argocd/argocd-repo-server,Depth:28,Reason:slow event handlers blocking the queue (04-Jul-2024 15:18:59.989) (total time: 203ms):
Trace[1015974457]: [203.949238ms] [203.949238ms] END
I0704 15:19:05.489867       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="my-grafana/grafana-867556d6f5-tljgt" err="Timeout: request did not complete within requested timeout - context deadline exceeded"
E0704 15:19:05.196939       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" logger="horizontal-pod-autoscaler-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook.17df0b2550b91423  tekton-pipelines   202501 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:HorizontalPodAutoscaler,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook,UID:a8bec02f-410c-4412-be4d-8571f8b3a5e0,APIVersion:autoscaling/v2,ResourceVersion:198820,FieldPath:,},Reason:FailedComputeMetricsReplicas,Message:invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io),Source:EventSource{Component:horizontal-pod-autoscaler,Host:,},FirstTimestamp:2024-07-04 15:10:37 +0000 UTC,LastTimestamp:2024-07-04 15:17:18.209760013 +0000 UTC m=+452.100656208,Count:6,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:horizontal-pod-autoscaler,ReportingInstance:,}"
E0704 15:19:06.493691       1 horizontal.go:270] failed to compute desired number of replicas based on listed metrics for Deployment/tekton-pipelines/tekton-pipelines-webhook: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
I0704 15:19:10.596973       1 request.go:697] Waited for 1.21028505s due to client-side throttling, not priority and fairness, request: GET:https://192.168.58.2:8443/api
I0704 15:19:51.173375       1 endpointslice_controller.go:311] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="tekton-pipelines/tekton-pipelines-webhook" err="failed to update tekton-pipelines-webhook-hx6br EndpointSlice for Service tekton-pipelines/tekton-pipelines-webhook: etcdserver: request timed out"
I0704 15:19:51.172020       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="argocd/postgres-0" err="etcdserver: request timed out"
I0704 15:19:51.179165       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"tekton-pipelines", Name:"tekton-pipelines-webhook", UID:"cb8f6e32-dbcd-458e-be49-837ea80661b7", APIVersion:"v1", ResourceVersion:"33499", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service tekton-pipelines/tekton-pipelines-webhook: failed to update tekton-pipelines-webhook-hx6br EndpointSlice for Service tekton-pipelines/tekton-pipelines-webhook: etcdserver: request timed out
E0704 15:19:51.075493       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook-644f45c89f-8q5n9.17df0ba26b4fa9d4  tekton-pipelines    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook-644f45c89f-8q5n9,UID:6691f57c-b365-491b-bed3-caa0bbf1de67,APIVersion:v1,ResourceVersion:202126,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2024-07-04 15:19:34.891809236 +0000 UTC m=+588.808070859,LastTimestamp:2024-07-04 15:19:34.891809236 +0000 UTC m=+588.808070859,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
E0704 15:20:05.775489       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpointslice-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook.17df0ba62faae02b  tekton-pipelines    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Service,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook,UID:cb8f6e32-dbcd-458e-be49-837ea80661b7,APIVersion:v1,ResourceVersion:33499,FieldPath:,},Reason:FailedToUpdateEndpointSlices,Message:Error updating Endpoint Slices for Service tekton-pipelines/tekton-pipelines-webhook: failed to update tekton-pipelines-webhook-hx6br EndpointSlice for Service tekton-pipelines/tekton-pipelines-webhook: etcdserver: request timed out,Source:EventSource{Component:endpoint-slice-controller,Host:,},FirstTimestamp:2024-07-04 15:19:51.071023147 +0000 UTC m=+605.000895721,LastTimestamp:2024-07-04 15:19:51.071023147 +0000 UTC m=+605.000895721,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-slice-controller,ReportingInstance:,}"
I0704 15:20:11.380277       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="prometheus/prometheus-kube-state-metrics-67848d7455-d7dpt" err="etcdserver: request timed out"
I0704 15:20:12.370409       1 endpointslice_controller.go:311] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="tekton-pipelines/tekton-pipelines-webhook" err="failed to update tekton-pipelines-webhook-hx6br EndpointSlice for Service tekton-pipelines/tekton-pipelines-webhook: etcdserver: request timed out"
I0704 15:20:12.382787       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"tekton-pipelines", Name:"tekton-pipelines-webhook", UID:"cb8f6e32-dbcd-458e-be49-837ea80661b7", APIVersion:"v1", ResourceVersion:"33499", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service tekton-pipelines/tekton-pipelines-webhook: failed to update tekton-pipelines-webhook-hx6br EndpointSlice for Service tekton-pipelines/tekton-pipelines-webhook: etcdserver: request timed out
E0704 15:20:12.478918       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="node-lifecycle-controller" event="&Event{ObjectMeta:{postgres-0.17df0ba63bcf8613  argocd    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Pod,Namespace:argocd,Name:postgres-0,UID:e77cb3e8-f1fa-4e3e-8429-42c8b2e20389,APIVersion:v1,ResourceVersion:202079,FieldPath:,},Reason:NodeNotReady,Message:Node is not ready,Source:EventSource{Component:node-controller,Host:,},FirstTimestamp:2024-07-04 15:19:51.274751507 +0000 UTC m=+605.204623948,LastTimestamp:2024-07-04 15:19:51.274751507 +0000 UTC m=+605.204623948,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:node-controller,ReportingInstance:,}"
E0704 15:20:13.582253       1 event.go:359] "Server rejected event (will not retry!)" err="Timeout: request did not complete within requested timeout - context deadline exceeded" logger="horizontal-pod-autoscaler-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook.17df0b2550b91423  tekton-pipelines   202501 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:HorizontalPodAutoscaler,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook,UID:a8bec02f-410c-4412-be4d-8571f8b3a5e0,APIVersion:autoscaling/v2,ResourceVersion:198820,FieldPath:,},Reason:FailedComputeMetricsReplicas,Message:invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io),Source:EventSource{Component:horizontal-pod-autoscaler,Host:,},FirstTimestamp:2024-07-04 15:10:37 +0000 UTC,LastTimestamp:2024-07-04 15:19:05.802322888 +0000 UTC m=+559.712608353,Count:7,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:horizontal-pod-autoscaler,ReportingInstance:,}"
E0704 15:20:21.281138       1 event.go:359] "Server rejected event (will not retry!)" err="etcdserver: request timed out" logger="endpoints-controller" event="&Event{ObjectMeta:{tekton-pipelines-webhook.17df0ba629b3b356  tekton-pipelines    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Endpoints,Namespace:tekton-pipelines,Name:tekton-pipelines-webhook,UID:e37dc2eb-1492-4bf0-9455-453ce393b7e5,APIVersion:v1,ResourceVersion:202229,FieldPath:,},Reason:FailedToUpdateEndpoint,Message:Failed to update endpoint tekton-pipelines/tekton-pipelines-webhook: etcdserver: request timed out,Source:EventSource{Component:endpoint-controller,Host:,},FirstTimestamp:2024-07-04 15:19:50.970938198 +0000 UTC m=+604.900810711,LastTimestamp:2024-07-04 15:19:50.970938198 +0000 UTC m=+604.900810711,Count:1,Type:Warning,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:endpoint-controller,ReportingInstance:,}"
I0704 15:20:21.972007       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="tekton-pipelines/tekton-events-controller-56b58fcf8b-p9xps" err="etcdserver: request timed out"


==> kube-proxy [b433a037a1f8] <==
I0704 15:00:35.024557       1 trace.go:236] Trace[1571624247]: "DeltaFIFO Pop Process" ID:default/kubernetes,Depth:18,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:33.123) (total time: 310ms):
Trace[1571624247]: [310.919912ms] [310.919912ms] END
I0704 15:00:37.429939       1 trace.go:236] Trace[395354179]: "DeltaFIFO Pop Process" ID:ingress-nginx/ingress-nginx-controller-f85zx,Depth:13,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:35.636) (total time: 490ms):
Trace[395354179]: [490.422303ms] [490.422303ms] END
I0704 15:00:37.429753       1 trace.go:236] Trace[1014954747]: "DeltaFIFO Pop Process" ID:default/postgres,Depth:17,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:35.625) (total time: 1009ms):
Trace[1014954747]: [1.009982734s] [1.009982734s] END
I0704 15:00:41.523060       1 trace.go:236] Trace[555609700]: "DeltaFIFO Pop Process" ID:kube-system/kube-dns-md29d,Depth:12,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:38.725) (total time: 1601ms):
Trace[555609700]: [1.601254807s] [1.601254807s] END
I0704 15:00:41.522975       1 trace.go:236] Trace[630916008]: "DeltaFIFO Pop Process" ID:default/rails,Depth:16,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:38.633) (total time: 1693ms):
Trace[630916008]: [1.693393697s] [1.693393697s] END
I0704 15:00:44.720532       1 trace.go:236] Trace[1784748135]: "DeltaFIFO Pop Process" ID:ingress-nginx/ingress-nginx-controller,Depth:15,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:42.418) (total time: 911ms):
Trace[1784748135]: [911.243762ms] [911.243762ms] END
I0704 15:00:44.719756       1 trace.go:236] Trace[1581868323]: "DeltaFIFO Pop Process" ID:my-grafana/grafana-dmd59,Depth:11,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:42.418) (total time: 900ms):
Trace[1581868323]: [900.048991ms] [900.048991ms] END
I0704 15:00:49.829197       1 trace.go:236] Trace[489975670]: "iptables ChainExists" (04-Jul-2024 15:00:46.325) (total time: 3503ms):
Trace[489975670]: [3.503598742s] [3.503598742s] END
I0704 15:00:49.821462       1 trace.go:236] Trace[1859197415]: "DeltaFIFO Pop Process" ID:ingress-nginx/ingress-nginx-controller-admission,Depth:14,Reason:slow event handlers blocking the queue (04-Jul-2024 15:00:46.519) (total time: 1710ms):
Trace[1859197415]: [1.710576113s] [1.710576113s] END
I0704 15:00:50.125721       1 trace.go:236] Trace[56515044]: "iptables ChainExists" (04-Jul-2024 15:00:46.327) (total time: 3798ms):
Trace[56515044]: [3.798306608s] [3.798306608s] END
I0704 15:01:08.124513       1 trace.go:236] Trace[1406107195]: "iptables save" (04-Jul-2024 15:00:55.420) (total time: 9910ms):
Trace[1406107195]: [9.91036801s] [9.91036801s] END
I0704 15:01:19.518373       1 trace.go:236] Trace[454634331]: "iptables ChainExists" (04-Jul-2024 15:01:16.013) (total time: 3005ms):
Trace[454634331]: [3.005994837s] [3.005994837s] END
I0704 15:01:19.519017       1 trace.go:236] Trace[1216080837]: "iptables restore" (04-Jul-2024 15:01:12.722) (total time: 6491ms):
Trace[1216080837]: [6.491977162s] [6.491977162s] END
I0704 15:01:33.122723       1 trace.go:236] Trace[1221509259]: "iptables ChainExists" (04-Jul-2024 15:01:19.924) (total time: 9397ms):
Trace[1221509259]: [9.397283746s] [9.397283746s] END
I0704 15:01:56.517261       1 trace.go:236] Trace[1985875937]: "iptables ChainExists" (04-Jul-2024 15:01:46.217) (total time: 8797ms):
Trace[1985875937]: [8.797364326s] [8.797364326s] END
I0704 15:01:56.721151       1 trace.go:236] Trace[1506803595]: "iptables ChainExists" (04-Jul-2024 15:01:46.315) (total time: 8600ms):
Trace[1506803595]: [8.600804649s] [8.600804649s] END
I0704 15:02:26.412156       1 trace.go:236] Trace[1623665901]: "iptables ChainExists" (04-Jul-2024 15:02:16.114) (total time: 8200ms):
Trace[1623665901]: [8.200868058s] [8.200868058s] END
I0704 15:02:26.211489       1 trace.go:236] Trace[598784641]: "iptables ChainExists" (04-Jul-2024 15:02:16.314) (total time: 8002ms):
Trace[598784641]: [8.002334678s] [8.002334678s] END
I0704 15:02:55.003016       1 trace.go:236] Trace[1726810118]: "iptables ChainExists" (04-Jul-2024 15:02:46.714) (total time: 6198ms):
Trace[1726810118]: [6.198611464s] [6.198611464s] END
I0704 15:02:55.002775       1 trace.go:236] Trace[1045443995]: "iptables save" (04-Jul-2024 15:02:45.406) (total time: 9401ms):
Trace[1045443995]: [9.401684578s] [9.401684578s] END
I0704 15:03:01.009816       1 trace.go:236] Trace[551720]: "iptables ChainExists" (04-Jul-2024 15:02:55.316) (total time: 4101ms):
Trace[551720]: [4.101049259s] [4.101049259s] END
I0704 15:03:13.197651       1 trace.go:236] Trace[2038245820]: "iptables restore" (04-Jul-2024 15:03:01.702) (total time: 9805ms):
Trace[2038245820]: [9.805593273s] [9.805593273s] END
I0704 15:03:23.995644       1 trace.go:236] Trace[1298121370]: "iptables ChainExists" (04-Jul-2024 15:03:16.695) (total time: 6498ms):
Trace[1298121370]: [6.498391173s] [6.498391173s] END
I0704 15:03:24.000075       1 trace.go:236] Trace[355205466]: "iptables ChainExists" (04-Jul-2024 15:03:16.401) (total time: 6795ms):
Trace[355205466]: [6.795737914s] [6.795737914s] END
I0704 15:03:46.889251       1 trace.go:236] Trace[1615462967]: "iptables save" (04-Jul-2024 15:03:27.292) (total time: 18906ms):
Trace[1615462967]: [18.906957317s] [18.906957317s] END
I0704 15:03:53.693581       1 trace.go:236] Trace[1267878621]: "iptables ChainExists" (04-Jul-2024 15:03:48.092) (total time: 4391ms):
Trace[1267878621]: [4.391748756s] [4.391748756s] END
I0704 15:03:53.792148       1 trace.go:236] Trace[1570139548]: "iptables ChainExists" (04-Jul-2024 15:03:45.991) (total time: 6203ms):
Trace[1570139548]: [6.203351931s] [6.203351931s] END
I0704 15:04:11.589798       1 trace.go:236] Trace[1696469915]: "iptables restore" (04-Jul-2024 15:03:54.384) (total time: 12703ms):
Trace[1696469915]: [12.703494007s] [12.703494007s] END
I0704 15:04:28.390502       1 trace.go:236] Trace[439993035]: "iptables ChainExists" (04-Jul-2024 15:04:18.492) (total time: 8090ms):
Trace[439993035]: [8.090600006s] [8.090600006s] END
I0704 15:04:28.382218       1 trace.go:236] Trace[500788022]: "iptables ChainExists" (04-Jul-2024 15:04:18.993) (total time: 8691ms):
Trace[500788022]: [8.691675085s] [8.691675085s] END


==> kube-proxy [e1c0a8e431cb] <==
Trace[636677827]: ---"Objects listed" error:<nil> 86698ms (15:15:46.032)
Trace[636677827]: [1m27.399824806s] [1m27.399824806s] END
I0704 15:15:47.625899       1 trace.go:236] Trace[832824321]: "DeltaFIFO Pop Process" ID:argocd/argocd-applicationset-controller-slhx5,Depth:27,Reason:slow event handlers blocking the queue (04-Jul-2024 15:15:46.723) (total time: 101ms):
Trace[832824321]: [101.937214ms] [101.937214ms] END
I0704 15:15:47.832348       1 trace.go:236] Trace[122884510]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (04-Jul-2024 15:14:19.436) (total time: 87298ms):
Trace[122884510]: ---"Objects listed" error:<nil> 85902ms (15:15:45.338)
Trace[122884510]: [1m27.29852014s] [1m27.29852014s] END
I0704 15:15:47.929452       1 trace.go:236] Trace[1985960620]: "DeltaFIFO Pop Process" ID:argocd/argocd-notifications-controller-metrics,Depth:25,Reason:slow event handlers blocking the queue (04-Jul-2024 15:15:47.827) (total time: 101ms):
Trace[1985960620]: [101.375098ms] [101.375098ms] END
I0704 15:15:47.932092       1 trace.go:236] Trace[1115554795]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (04-Jul-2024 15:14:18.831) (total time: 87900ms):
Trace[1115554795]: ---"Objects listed" error:<nil> 85900ms (15:15:44.730)
Trace[1115554795]: [1m27.90062216s] [1m27.90062216s] END
I0704 15:15:48.021556       1 trace.go:236] Trace[1646380826]: "DeltaFIFO Pop Process" ID:argocd/argocd-dex-server-d7z45,Depth:26,Reason:slow event handlers blocking the queue (04-Jul-2024 15:15:47.830) (total time: 103ms):
Trace[1646380826]: [103.172514ms] [103.172514ms] END
I0704 15:15:54.826379       1 trace.go:236] Trace[605861187]: "iptables ChainExists" (04-Jul-2024 15:15:50.228) (total time: 2999ms):
Trace[605861187]: [2.999715381s] [2.999715381s] END
I0704 15:15:54.723533       1 trace.go:236] Trace[1203684310]: "iptables ChainExists" (04-Jul-2024 15:15:50.232) (total time: 2996ms):
Trace[1203684310]: [2.996114162s] [2.996114162s] END
I0704 15:16:26.025252       1 trace.go:236] Trace[1110912307]: "iptables ChainExists" (04-Jul-2024 15:16:20.234) (total time: 4295ms):
Trace[1110912307]: [4.295898964s] [4.295898964s] END
I0704 15:16:26.027438       1 trace.go:236] Trace[350403920]: "iptables ChainExists" (04-Jul-2024 15:16:20.337) (total time: 4493ms):
Trace[350403920]: [4.493464324s] [4.493464324s] END
I0704 15:16:58.817875       1 trace.go:236] Trace[1723754506]: "iptables ChainExists" (04-Jul-2024 15:16:50.727) (total time: 7392ms):
Trace[1723754506]: [7.392681392s] [7.392681392s] END
I0704 15:16:58.817910       1 trace.go:236] Trace[1065094343]: "iptables ChainExists" (04-Jul-2024 15:16:50.727) (total time: 7200ms):
Trace[1065094343]: [7.200641913s] [7.200641913s] END
I0704 15:17:10.325319       1 trace.go:236] Trace[1205587029]: "iptables save" (04-Jul-2024 15:16:59.318) (total time: 9505ms):
Trace[1205587029]: [9.50532265s] [9.50532265s] END
I0704 15:17:27.209762       1 trace.go:236] Trace[1531408992]: "iptables ChainExists" (04-Jul-2024 15:17:20.624) (total time: 4291ms):
Trace[1531408992]: [4.291231129s] [4.291231129s] END
I0704 15:17:27.209886       1 trace.go:236] Trace[2147450706]: "iptables restore" (04-Jul-2024 15:17:16.022) (total time: 9995ms):
Trace[2147450706]: [9.995945265s] [9.995945265s] END
I0704 15:17:37.122407       1 trace.go:236] Trace[140000188]: "iptables ChainExists" (04-Jul-2024 15:17:28.318) (total time: 6496ms):
Trace[140000188]: [6.496451123s] [6.496451123s] END
I0704 15:17:58.208145       1 trace.go:236] Trace[826059957]: "iptables ChainExists" (04-Jul-2024 15:17:50.410) (total time: 6398ms):
Trace[826059957]: [6.39871407s] [6.39871407s] END
I0704 15:17:58.405866       1 trace.go:236] Trace[253607189]: "iptables ChainExists" (04-Jul-2024 15:17:50.502) (total time: 6306ms):
Trace[253607189]: [6.306866954s] [6.306866954s] END
I0704 15:18:27.800002       1 trace.go:236] Trace[2050119785]: "iptables ChainExists" (04-Jul-2024 15:18:20.999) (total time: 5505ms):
Trace[2050119785]: [5.505989231s] [5.505989231s] END
I0704 15:18:27.803215       1 trace.go:236] Trace[1940461761]: "iptables ChainExists" (04-Jul-2024 15:18:20.805) (total time: 5697ms):
Trace[1940461761]: [5.697251395s] [5.697251395s] END
I0704 15:18:55.499076       1 trace.go:236] Trace[645666875]: "iptables ChainExists" (04-Jul-2024 15:18:50.302) (total time: 4200ms):
Trace[645666875]: [4.200166783s] [4.200166783s] END
I0704 15:18:55.601693       1 trace.go:236] Trace[1184749863]: "iptables ChainExists" (04-Jul-2024 15:18:50.393) (total time: 4103ms):
Trace[1184749863]: [4.1038338s] [4.1038338s] END
I0704 15:19:03.893096       1 trace.go:236] Trace[1193518098]: "iptables save" (04-Jul-2024 15:18:56.098) (total time: 5393ms):
Trace[1193518098]: [5.393398798s] [5.393398798s] END
I0704 15:19:27.396945       1 trace.go:236] Trace[190523124]: "iptables ChainExists" (04-Jul-2024 15:19:20.989) (total time: 5005ms):
Trace[190523124]: [5.005511937s] [5.005511937s] END
I0704 15:19:27.487498       1 trace.go:236] Trace[715231845]: "iptables restore" (04-Jul-2024 15:19:13.889) (total time: 13205ms):
Trace[715231845]: [13.205388007s] [13.205388007s] END
I0704 15:19:32.700996       1 trace.go:236] Trace[247782777]: "iptables ChainExists" (04-Jul-2024 15:19:27.786) (total time: 2901ms):
Trace[247782777]: [2.901664995s] [2.901664995s] END
I0704 15:19:56.372041       1 trace.go:236] Trace[1634866529]: "iptables ChainExists" (04-Jul-2024 15:19:50.170) (total time: 4812ms):
Trace[1634866529]: [4.812182226s] [4.812182226s] END
I0704 15:19:56.471172       1 trace.go:236] Trace[477012831]: "iptables ChainExists" (04-Jul-2024 15:19:50.172) (total time: 4999ms):
Trace[477012831]: [4.999216393s] [4.999216393s] END
I0704 15:20:22.878658       1 trace.go:236] Trace[1017496368]: "iptables ChainExists" (04-Jul-2024 15:20:20.280) (total time: 2099ms):
Trace[1017496368]: [2.099277857s] [2.099277857s] END


==> kube-scheduler [267622c2176e] <==
W0704 15:09:04.566683       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.566784       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.633375       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.633438       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.678776       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.678884       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.687130       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.687296       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.688307       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.688426       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.691823       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.692133       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.766288       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.766495       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.824465       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.824528       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.867440       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:04.867404       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.867763       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0704 15:09:04.867792       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0704 15:09:14.977969       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0704 15:09:14.980255       1 trace.go:236] Trace[939813537]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (04-Jul-2024 15:09:04.975) (total time: 10002ms):
Trace[939813537]: ---"Objects listed" error:Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (15:09:14.977)
Trace[939813537]: [10.002062422s] [10.002062422s] END
E0704 15:09:14.980334       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0704 15:09:15.005612       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0704 15:09:15.005670       1 trace.go:236] Trace[1570589926]: "Reflector ListAndWatch" name:k8s.io/client-go/informers/factory.go:160 (04-Jul-2024 15:09:05.003) (total time: 10002ms):
Trace[1570589926]: ---"Objects listed" error:Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (15:09:15.005)
Trace[1570589926]: [10.002116398s] [10.002116398s] END
E0704 15:09:15.005684       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0704 15:09:16.067759       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0704 15:09:16.067836       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0704 15:09:16.067904       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0704 15:09:16.067925       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0704 15:09:16.067789       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0704 15:09:16.067962       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0704 15:09:16.067834       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0704 15:09:16.067981       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0704 15:09:16.068042       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0704 15:09:16.068069       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0704 15:09:16.068592       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0704 15:09:16.068648       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0704 15:09:16.072730       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0704 15:09:16.072788       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0704 15:09:16.072730       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0704 15:09:16.072831       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0704 15:09:16.072825       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0704 15:09:16.072875       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0704 15:09:16.072962       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0704 15:09:16.072992       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0704 15:09:16.073985       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0704 15:09:16.074022       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0704 15:09:16.075033       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0704 15:09:16.080198       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0704 15:09:16.169978       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0704 15:09:16.169616       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
I0704 15:09:21.772408       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0704 15:20:31.035374       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0704 15:20:31.071924       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0704 15:20:31.193164       1 run.go:74] "command failed" err="finished without leader elect"


==> kube-scheduler [d892d4f498c3] <==
command /bin/bash -c "docker logs --tail 60 d892d4f498c3" failed with error: /bin/bash -c "docker logs --tail 60 d892d4f498c3": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: d892d4f498c3


==> kubelet <==
Jul 04 15:21:08 minikube kubelet[1500]: I0704 15:21:08.767187    1500 scope.go:117] "RemoveContainer" containerID="2041788cc5ecedecd7cf43e2f40815229d6394fecf316d122584d2f19fc193c2"
Jul 04 15:21:08 minikube kubelet[1500]: W0704 15:21:08.884747    1500 reflector.go:547] object-"kube-system"/"kube-proxy": failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)kube-proxy&resourceVersion=202399": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:08 minikube kubelet[1500]: E0704 15:21:08.885022    1500 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)kube-proxy&resourceVersion=202399": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:08 minikube kubelet[1500]: W0704 15:21:08.950592    1500 reflector.go:547] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)coredns&resourceVersion=202399": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:08 minikube kubelet[1500]: E0704 15:21:08.950690    1500 reflector.go:150] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)coredns&resourceVersion=202399": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.149634    1500 scope.go:117] "RemoveContainer" containerID="05b66715c006b156132c5da0ef39a996807dfd7b816ed1492cbf99fb63526eb2"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.289274    1500 scope.go:117] "RemoveContainer" containerID="9f1989093f0299646863af208e60849f7802e02ca43d29182443b8422565c660"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.479325    1500 scope.go:117] "RemoveContainer" containerID="d1a872eab66d447fa6b896e72b8abe7f5e97f7308653befa42e31d5ff8cacdfb"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.479601    1500 status_manager.go:853] "Failed to get status for pod" podUID="1df06efb-68da-483f-9683-7d7217cf00d4" pod="argocd/argocd-notifications-controller-6bbd5dd8d-sv2z6" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/argocd/pods/argocd-notifications-controller-6bbd5dd8d-sv2z6\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: E0704 15:21:09.479718    1500 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 20s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(a341953c-e697-4b87-bb48-1a98751cb978)\"" pod="kube-system/storage-provisioner" podUID="a341953c-e697-4b87-bb48-1a98751cb978"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.480011    1500 status_manager.go:853] "Failed to get status for pod" podUID="e5016c0f-b59d-4e10-b264-c9d5ca4dbdd9" pod="my-grafana/grafana-867556d6f5-tljgt" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/my-grafana/pods/grafana-867556d6f5-tljgt\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.480287    1500 status_manager.go:853] "Failed to get status for pod" podUID="2c115b66-daca-45da-8f52-59fc5a7e5657" pod="prometheus/prometheus-prometheus-node-exporter-k8wg8" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prometheus/pods/prometheus-prometheus-node-exporter-k8wg8\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.480664    1500 status_manager.go:853] "Failed to get status for pod" podUID="b419d179-8cc3-4a27-81e1-4cef51f534ec" pod="default/postgres-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/postgres-0\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.481383    1500 status_manager.go:853] "Failed to get status for pod" podUID="14147cc9-7ba7-4e83-8a50-7bb2e8fad632" pod="argocd/argocd-application-controller-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/argocd/pods/argocd-application-controller-0\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.482122    1500 status_manager.go:853] "Failed to get status for pod" podUID="c997f21b-8ac8-4ae0-89e6-46f58fafba4a" pod="prometheus/prometheus-alertmanager-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prometheus/pods/prometheus-alertmanager-0\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.483515    1500 status_manager.go:853] "Failed to get status for pod" podUID="2c0b6d18-6e66-4f89-a523-ddecdf275459" pod="prometheus/prometheus-6f6ddc8745-d4gs4" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prometheus/pods/prometheus-6f6ddc8745-d4gs4\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.483983    1500 status_manager.go:853] "Failed to get status for pod" podUID="d2451fc1-7eb4-4f48-bcc4-4ca38dfbea63" pod="ingress-nginx/ingress-nginx-controller-84df5799c-kf49k" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/ingress-nginx/pods/ingress-nginx-controller-84df5799c-kf49k\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.484429    1500 status_manager.go:853] "Failed to get status for pod" podUID="2fdeb80a-d8fb-4f38-af0a-501cfc155fa9" pod="tekton-pipelines-resolvers/tekton-pipelines-remote-resolvers-659495bc6-9kpvm" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines-resolvers/pods/tekton-pipelines-remote-resolvers-659495bc6-9kpvm\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.485220    1500 status_manager.go:853] "Failed to get status for pod" podUID="862a1f5a-039a-438e-b549-c4d683351bb2" pod="kube-system/coredns-7db6d8ff4d-4fw89" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-7db6d8ff4d-4fw89\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.485791    1500 status_manager.go:853] "Failed to get status for pod" podUID="3c02d130-2507-48ff-b23e-dbde4221c87f" pod="argocd/argocd-repo-server-f965fdfcf-fjjnx" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/argocd/pods/argocd-repo-server-f965fdfcf-fjjnx\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.486217    1500 status_manager.go:853] "Failed to get status for pod" podUID="63efe95f-ff60-4cfe-860c-9e4cbe50efc0" pod="argocd/argocd-server-5bdf56c97f-b9jkt" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/argocd/pods/argocd-server-5bdf56c97f-b9jkt\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.486700    1500 status_manager.go:853] "Failed to get status for pod" podUID="51d8e3d4-90d6-4b64-b212-bf18e5e5ad72" pod="tekton-pipelines/tekton-dashboard-65cdfdc6c6-9cnsl" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/pods/tekton-dashboard-65cdfdc6c6-9cnsl\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.487361    1500 status_manager.go:853] "Failed to get status for pod" podUID="f9c8e1d0d74b1727abdb4b4a31d3a7c1" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.488127    1500 status_manager.go:853] "Failed to get status for pod" podUID="3bcb3a373f1da5f7af51088428d5d964" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.488843    1500 status_manager.go:853] "Failed to get status for pod" podUID="a341953c-e697-4b87-bb48-1a98751cb978" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.489518    1500 status_manager.go:853] "Failed to get status for pod" podUID="71909e90-8d1e-431e-a1cb-cb9d6dea8e48" pod="kube-system/coredns-7db6d8ff4d-6mphs" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-7db6d8ff4d-6mphs\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.490370    1500 status_manager.go:853] "Failed to get status for pod" podUID="1bce2833-d95a-4bfd-969e-68c3b2fdffca" pod="prometheus/prometheus-kube-state-metrics-67848d7455-d7dpt" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prometheus/pods/prometheus-kube-state-metrics-67848d7455-d7dpt\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.490976    1500 status_manager.go:853] "Failed to get status for pod" podUID="a45ecab2-0324-400d-a3de-bd8079fe8a26" pod="prometheus/prometheus-server-5787759b8c-68cjv" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prometheus/pods/prometheus-server-5787759b8c-68cjv\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.491528    1500 status_manager.go:853] "Failed to get status for pod" podUID="7fd44e8d11c3e0ffe6b1825e2a1f2270" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.493204    1500 status_manager.go:853] "Failed to get status for pod" podUID="e77cb3e8-f1fa-4e3e-8429-42c8b2e20389" pod="argocd/postgres-0" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/argocd/pods/postgres-0\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.494111    1500 status_manager.go:853] "Failed to get status for pod" podUID="6691f57c-b365-491b-bed3-caa0bbf1de67" pod="tekton-pipelines/tekton-pipelines-webhook-644f45c89f-8q5n9" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/pods/tekton-pipelines-webhook-644f45c89f-8q5n9\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.494642    1500 status_manager.go:853] "Failed to get status for pod" podUID="da854a90-47cc-4595-8b75-d8ed6cd45405" pod="tekton-pipelines/tekton-events-controller-56b58fcf8b-p9xps" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/pods/tekton-events-controller-56b58fcf8b-p9xps\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.495427    1500 status_manager.go:853] "Failed to get status for pod" podUID="d2265cf7-4c16-42a3-a480-f04c07965e28" pod="tekton-pipelines/tekton-pipelines-controller-c5bc687cf-7dqks" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/pods/tekton-pipelines-controller-c5bc687cf-7dqks\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.496029    1500 status_manager.go:853] "Failed to get status for pod" podUID="245409ee-8b06-4d83-bdfa-d8693e1d8af5" pod="prometheus/prometheus-prometheus-pushgateway-58cb869bcc-jbp7c" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prometheus/pods/prometheus-prometheus-pushgateway-58cb869bcc-jbp7c\": dial tcp 192.168.58.2:8443: connect: connection refused"
Jul 04 15:21:09 minikube kubelet[1500]: W0704 15:21:09.668222    1500 reflector.go:547] object-"tekton-pipelines"/"config-logging": failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/configmaps?fieldSelector=metadata.name%!D(MISSING)config-logging&resourceVersion=202464": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:09 minikube kubelet[1500]: W0704 15:21:09.668323    1500 reflector.go:547] object-"tekton-pipelines"/"config-registry-cert": failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/configmaps?fieldSelector=metadata.name%!D(MISSING)config-registry-cert&resourceVersion=202464": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:09 minikube kubelet[1500]: E0704 15:21:09.668433    1500 reflector.go:150] object-"tekton-pipelines"/"config-logging": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/configmaps?fieldSelector=metadata.name%!D(MISSING)config-logging&resourceVersion=202464": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:09 minikube kubelet[1500]: E0704 15:21:09.668458    1500 reflector.go:150] object-"tekton-pipelines"/"config-registry-cert": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://control-plane.minikube.internal:8443/api/v1/namespaces/tekton-pipelines/configmaps?fieldSelector=metadata.name%!D(MISSING)config-registry-cert&resourceVersion=202464": dial tcp 192.168.58.2:8443: connect: connection refused
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.674219    1500 scope.go:117] "RemoveContainer" containerID="998b9fc766c671040deb564be0f3259c982837113b7680e4c5a45fa8531457d3"
Jul 04 15:21:09 minikube kubelet[1500]: E0704 15:21:09.696040    1500 desired_state_of_world_populator.go:318] "Error processing volume" err="error processing PVC my-grafana/grafana-pvc: failed to fetch PVC from API server: Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/my-grafana/persistentvolumeclaims/grafana-pvc\": dial tcp 192.168.58.2:8443: connect: connection refused" pod="my-grafana/grafana-867556d6f5-tljgt" volumeName="grafana-pv"
Jul 04 15:21:09 minikube kubelet[1500]: I0704 15:21:09.886819    1500 scope.go:117] "RemoveContainer" containerID="a3e1461cff256caff612dfe59a6041fce0bd155612044fd32ed15ecffacfb0d4"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.086780    1500 scope.go:117] "RemoveContainer" containerID="d5aebd2fd2ca626bfce49958a80880f312aa121988ff3d412a40f1c481126203"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.279369    1500 scope.go:117] "RemoveContainer" containerID="e1e7a64d2aed974542621fae31c3c5d5392f15d2664ff59304209539a61fd66d"
Jul 04 15:21:10 minikube kubelet[1500]: E0704 15:21:10.280778    1500 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"tekton-events-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=tekton-events-controller pod=tekton-events-controller-56b58fcf8b-p9xps_tekton-pipelines(da854a90-47cc-4595-8b75-d8ed6cd45405)\"" pod="tekton-pipelines/tekton-events-controller-56b58fcf8b-p9xps" podUID="da854a90-47cc-4595-8b75-d8ed6cd45405"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.379690    1500 scope.go:117] "RemoveContainer" containerID="31a2c82fe9e7314309f5cc29a5d6edb1ef547d15ab3ad5ee4346be84988f3f20"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.688333    1500 scope.go:117] "RemoveContainer" containerID="61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f"
Jul 04 15:21:10 minikube kubelet[1500]: E0704 15:21:10.691080    1500 configmap.go:199] Couldn't get configMap prometheus/prometheus-alertmanager: failed to sync configmap cache: timed out waiting for the condition
Jul 04 15:21:10 minikube kubelet[1500]: E0704 15:21:10.691238    1500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/c997f21b-8ac8-4ae0-89e6-46f58fafba4a-config podName:c997f21b-8ac8-4ae0-89e6-46f58fafba4a nodeName:}" failed. No retries permitted until 2024-07-04 15:21:26.691205553 +0000 UTC m=+758.093005467 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "config" (UniqueName: "kubernetes.io/configmap/c997f21b-8ac8-4ae0-89e6-46f58fafba4a-config") pod "prometheus-alertmanager-0" (UID: "c997f21b-8ac8-4ae0-89e6-46f58fafba4a") : failed to sync configmap cache: timed out waiting for the condition
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.708599    1500 scope.go:117] "RemoveContainer" containerID="eea27eeac451375b192cb516fd62602e53047048166566730e8b55761c9a0171"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.900465    1500 scope.go:117] "RemoveContainer" containerID="2669199b98ff78b9aa297387ef38788b7dc7aba34171d2169e2dd31bd8a531da"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.967484    1500 scope.go:117] "RemoveContainer" containerID="163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77"
Jul 04 15:21:10 minikube kubelet[1500]: I0704 15:21:10.967901    1500 scope.go:117] "RemoveContainer" containerID="ac9fd0cc297f1459b910078a10d5c887c725629e867dbae9da57eb95d6dbad99"
Jul 04 15:21:10 minikube kubelet[1500]: E0704 15:21:10.969873    1500 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"tekton-pipelines-controller\" with CrashLoopBackOff: \"back-off 10s restarting failed container=tekton-pipelines-controller pod=tekton-pipelines-controller-c5bc687cf-7dqks_tekton-pipelines(d2265cf7-4c16-42a3-a480-f04c07965e28)\"" pod="tekton-pipelines/tekton-pipelines-controller-c5bc687cf-7dqks" podUID="d2265cf7-4c16-42a3-a480-f04c07965e28"
Jul 04 15:21:11 minikube kubelet[1500]: E0704 15:21:10.991546    1500 remote_runtime.go:366] "StopContainer from runtime service failed" err="rpc error: code = NotFound desc = Error response from daemon: No such container: 61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f" containerID="61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f"
Jul 04 15:21:11 minikube kubelet[1500]: I0704 15:21:10.992919    1500 scope.go:117] "RemoveContainer" containerID="61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f"
Jul 04 15:21:11 minikube kubelet[1500]: E0704 15:21:11.069708    1500 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f" containerID="61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f"
Jul 04 15:21:11 minikube kubelet[1500]: E0704 15:21:11.072745    1500 kuberuntime_gc.go:150] "Failed to remove container" err="failed to get container status \"61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f\": rpc error: code = Unknown desc = Error response from daemon: No such container: 61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f" containerID="61179c7bfbc71dac59d88c60d45d2a7082a9c9786401d000c756507ce0b9601f"
Jul 04 15:21:11 minikube kubelet[1500]: I0704 15:21:11.072859    1500 scope.go:117] "RemoveContainer" containerID="163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77"
Jul 04 15:21:11 minikube kubelet[1500]: E0704 15:21:11.096064    1500 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77\": Error response from daemon: removal of container 163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77 is already in progress" containerID="163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77"
Jul 04 15:21:11 minikube kubelet[1500]: E0704 15:21:11.096145    1500 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77\": Error response from daemon: removal of container 163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77 is already in progress" containerID="163a9b04452b96ffe3f044151387f4ae7069e38372b7fb5afb7d4c3ac0341c77"


==> storage-provisioner [2041788cc5ec] <==
command /bin/bash -c "docker logs --tail 60 2041788cc5ec" failed with error: /bin/bash -c "docker logs --tail 60 2041788cc5ec": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: 2041788cc5ec


==> storage-provisioner [d5aebd2fd2ca] <==
command /bin/bash -c "docker logs --tail 60 d5aebd2fd2ca" failed with error: /bin/bash -c "docker logs --tail 60 d5aebd2fd2ca": Process exited with status 1
stdout:

stderr:
Error response from daemon: No such container: d5aebd2fd2ca

